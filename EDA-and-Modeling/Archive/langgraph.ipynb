{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9604da8",
   "metadata": {},
   "source": [
    "# Run Local Verif.AI Workflow (This version deprecated)\n",
    "\n",
    "- Author(s): Michael Denton and Cole Agard\n",
    "- Published: March 4, 2024\n",
    "\n",
    "This notebook connects to a local instance of a LLAMA-2 model and runs the Verif.AI workflow to serve the following uses:\n",
    "- 1.) Where applicable, extracts verifiable claims from a body of text\n",
    "- 2.) Searches via Google API the WWW for these evidence claims\n",
    "- 3.) Provides justification and list of sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eb6a23",
   "metadata": {},
   "source": [
    "To connect, ensure the following:\n",
    "\n",
    "- Langchain and Langgraph are set up and installed on the user's computer\n",
    "- Ollama is set up and installed and LLAMA-2 has been installed and is compatible\n",
    "    - This may require the user to have GPU with enough dedicated GPU memory to run the model\n",
    "    - Current local setup (M. Denton): Nvidia 1070 with 8GB dedicated memory\n",
    "    - CUDA set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae46b8f",
   "metadata": {},
   "source": [
    "##### Run a dummy prompt to ensure connection to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0bc72c8-f0d1-4dc2-91a5-f85071a5f831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why don't scientists trust atoms? Because they make up everything! ðŸ˜‚\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "chain = llm | output_parser\n",
    "\n",
    "print(chain.invoke(\"Tell me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05eb1f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trying out JSON\n",
    "\n",
    "# from typing import List\n",
    "\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain_core.output_parsers import JsonOutputParser\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# class Joke(BaseModel):\n",
    "#     setup: str = Field(description=\"question to set up a joke\")\n",
    "#     punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "# # And a query intented to prompt a language model to populate the data structure.\n",
    "# joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# # Set up a parser + inject instructions into the prompt template.\n",
    "# parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "# prompt = PromptTemplate(\n",
    "#     template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "#     input_variables=[\"query\"],\n",
    "#     partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "# )\n",
    "\n",
    "# chain = prompt | llm | parser\n",
    "\n",
    "# chain.invoke({\"query\": joke_query})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c5aafb",
   "metadata": {},
   "source": [
    "##### Define local function to extract verifiable claims\n",
    "- This has been deprecated in favor of a lang-graph solution but is a lang-chain implementation of a debate approach to finding verifable claims in a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e5d512a-f71e-4a12-bed5-2b93f0ba049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_verifiable_claims_via_debate(text, n_loops=1, n_agents=5):\n",
    "    \"\"\"\n",
    "    Given an input text, engages multiple agents in a debate to extract a list of verifiable and unverifable claims. Uses either 1 agent or five agents. If no loop, then just four agents (no reinforcer). Will be updated in future to use other multiples.\n",
    "    \"\"\"\n",
    "    from langchain_community.llms import Ollama\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "    # Get the LLM from Ollama\n",
    "    llm = Ollama(model=\"llama2\")\n",
    "\n",
    "    # Define the output parser\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    if n_agents == 1:\n",
    "        print('Only one agent specified, not running debate.')\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are tasked with extracting verifiable claims in a text.\"),\n",
    "            (\"system\", \"You answer in short sentences.\"),\n",
    "            (\"system\", \"Sentence in your answer is an extracted claim.\"),\n",
    "            (\"system\", \"The domain focus is politics. \"),\n",
    "            (\"system\", \"For each claim, you answer if it is verifiable or not and why.\"),\n",
    "            (\"user\", \"{input}\")\n",
    "        ])\n",
    "\n",
    "\n",
    "        # Define the output parser\n",
    "        output_parser = StrOutputParser()\n",
    "\n",
    "        # Define the chain\n",
    "        chain = prompt | llm | output_parser\n",
    "\n",
    "        return chain.invoke({\"input\": text})\n",
    "\n",
    "        \n",
    "    else:\n",
    "        if n_loops == 1:\n",
    "            \n",
    "            print('Defaulting to four agents')\n",
    "\n",
    "            # Searches for claims of figures\n",
    "            prompt_agent1 = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"You are tasked with extracting from text the claims of specific values, figures, or facts.\"),\n",
    "                (\"system\", \"You answer in short sentences.\"),\n",
    "                (\"system\", \"The domain focus is politics.\"),\n",
    "                (\"system\", \"Each reply is a bullet point.\"),\n",
    "                (\"user\", \"The text is: {input}\")\n",
    "            ])\n",
    "\n",
    "            # Searches for verifiable claims\n",
    "            prompt_agent2 = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"You are tasked with extracting from text the verifiable claims.\"),\n",
    "                (\"system\", \"You answer in short sentences.\"),\n",
    "                (\"system\", \"The domain focus is politics. \"),\n",
    "                (\"system\", \"Each reply is a bullet point.\"),\n",
    "                (\"user\", \"The text is: {input}\")\n",
    "            ])\n",
    "\n",
    "            # Searches for check-worthy claims\n",
    "            prompt_agent3 = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"You are tasked with extracting from text the opinions which are not verifiable.\"),\n",
    "                (\"system\", \"You answer in short sentences.\"),\n",
    "                (\"system\", \"The domain focus is politics. \"),\n",
    "                (\"system\", \"Each reply is a bullet point.\"),\n",
    "                (\"user\", \"The text is: {input}\")\n",
    "            ])\n",
    "\n",
    "            # Judge\n",
    "            prompt_agent4 = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"You are tasked with judging whether the following claims are contained within the text.\"),\n",
    "                (\"system\", \"Claims of facts are : {facts}\"),\n",
    "                (\"system\", \"Verifiable claims are: {verifclaims}\"),\n",
    "                (\"system\", \"Un-veriable opinions are: {opinions}\"),\n",
    "                (\"system\", \"These were judgements based on the following text: {input}\"),\n",
    "            ])\n",
    "\n",
    "            # Reinforcer\n",
    "            # prompt_agent5 = ChatPromptTemplate.from_messages([\n",
    "            #     (\"system\", \"Three agents have assessed a text for verifiable and unverifiable claims\"),\n",
    "            #     (\"system\", \"A judge has compared these to the original text and provided the feedback: {judgement}\"),\n",
    "            #     (\"user\", \"What changes should the agents implement?\"),\n",
    "            # ])\n",
    "\n",
    "\n",
    "            agent1_facts = prompt_agent1 | llm | output_parser\n",
    "            agent2_claims = prompt_agent2 | llm | output_parser\n",
    "            agent3_opinions = prompt_agent3 | llm | output_parser\n",
    "            agent4_judge = prompt_agent4 | llm | output_parser\n",
    "            # agent5_reinforcer = prompt_agent5 | llm | output_parser\n",
    "\n",
    "            agent1_feedback = agent1_facts.invoke({\"input\": text})\n",
    "            agent2_feedback = agent2_claims.invoke({\"input\": text})\n",
    "            agent3_feedback = agent3_opinions.invoke({\"input\": text})\n",
    "\n",
    "            agent4_feedback = agent4_judge.invoke(\n",
    "                {\n",
    "                    \"facts\":agent1_feedback, \n",
    "                    \"verifclaims\": agent2_feedback, \n",
    "                    \"opinions\":agent3_feedback, \n",
    "                    \"input\": text\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return agent4_feedback\n",
    "\n",
    "        else:\n",
    "\n",
    "            i=1\n",
    "            while i <= n_loops:\n",
    "                if i == 1:\n",
    "                    # Searches for claims of figures\n",
    "                    prompt_agent1 = ChatPromptTemplate.from_messages([\n",
    "                        (\"system\", \"You are tasked with extracting from text the claims of specific values, figures, or facts.\"),\n",
    "                        (\"system\", \"You answer in short sentences.\"),\n",
    "                        (\"system\", \"The domain focus is politics.\"),\n",
    "                        (\"system\", \"Each reply is a bullet point.\"),\n",
    "                        (\"user\", \"The text is: {input}\")\n",
    "                    ])\n",
    "\n",
    "                    # Searches for verifiable claims\n",
    "                    prompt_agent2 = ChatPromptTemplate.from_messages([\n",
    "                        (\"system\", \"You are tasked with extracting from text the verifiable claims.\"),\n",
    "                        (\"system\", \"You answer in short sentences.\"),\n",
    "                        (\"system\", \"The domain focus is politics. \"),\n",
    "                        (\"system\", \"Each reply is a bullet point.\"),\n",
    "                        (\"user\", \"The text is: {input}\")\n",
    "                    ])\n",
    "\n",
    "                    # Searches for check-worthy claims\n",
    "                    prompt_agent3 = ChatPromptTemplate.from_messages([\n",
    "                        (\"system\", \"You are tasked with extracting from text the opinions which are not verifiable.\"),\n",
    "                        (\"system\", \"You answer in short sentences.\"),\n",
    "                        (\"system\", \"The domain focus is politics. \"),\n",
    "                        (\"system\", \"Each reply is a bullet point.\"),\n",
    "                        (\"user\", \"The text is: {input}\")\n",
    "                    ])\n",
    "\n",
    "                    agent1_facts = prompt_agent1 | llm | output_parser\n",
    "                    agent2_claims = prompt_agent2 | llm | output_parser\n",
    "                    agent3_opinions = prompt_agent3 | llm | output_parser\n",
    "\n",
    "                    agent1_feedback = agent1_facts.invoke({\"input\": text})\n",
    "                    agent2_feedback = agent2_claims.invoke({\"input\": text})\n",
    "                    agent3_feedback = agent3_opinions.invoke({\"input\": text})\n",
    "\n",
    "                else:\n",
    "                    # Searches for claims of figures\n",
    "                    prompt_agent1 = ChatPromptTemplate.from_messages([\n",
    "                        (\"system\", \"You are tasked with extracting from text the claims of specific values, figures, or facts.\"),\n",
    "                        (\"system\", \"You answer in short sentences.\"),\n",
    "                        (\"system\", \"The domain focus is politics.\"),\n",
    "                        (\"system\", \"Each reply is a bullet point.\"),\n",
    "                        (\"system\", \"You have previously done this before, and a judge has suggested the following notes to improve your answer: {reinforcer_notes}\"),\n",
    "                        (\"user\", \"The main text is: {input}\")\n",
    "                    ])\n",
    "\n",
    "                    # Searches for verifiable claims\n",
    "                    prompt_agent2 = ChatPromptTemplate.from_messages([\n",
    "                        (\"system\", \"You are tasked with extracting from text the verifiable claims.\"),\n",
    "                        (\"system\", \"You answer in short sentences.\"),\n",
    "                        (\"system\", \"The domain focus is politics. \"),\n",
    "                        (\"system\", \"Each reply is a bullet point.\"),\n",
    "                        (\"system\", \"You have previously done this before, and a judge has suggested the following notes to improve your answer: {reinforcer_notes}\"),\n",
    "                        (\"user\", \"The main text is: {input}\")\n",
    "                    ])\n",
    "\n",
    "                    # Searches for check-worthy claims\n",
    "                    prompt_agent3 = ChatPromptTemplate.from_messages([\n",
    "                        (\"system\", \"You are tasked with extracting from text the opinions which are not verifiable.\"),\n",
    "                        (\"system\", \"You answer in short sentences.\"),\n",
    "                        (\"system\", \"The domain focus is politics. \"),\n",
    "                        (\"system\", \"Each reply is a bullet point.\"),\n",
    "                        (\"system\", \"You have previously done this before, and a judge has suggested the following notes to improve your answer: {reinforcer_notes}\"),\n",
    "                        (\"user\", \"The main text is: {input}\")\n",
    "                    ])                  \n",
    "\n",
    "                    agent1_facts = prompt_agent1 | llm | output_parser\n",
    "                    agent2_claims = prompt_agent2 | llm | output_parser\n",
    "                    agent3_opinions = prompt_agent3 | llm | output_parser\n",
    "\n",
    "                    agent1_feedback = agent1_facts.invoke({\"input\": text, \"reinforcer_notes\": agent5_reinforcer})\n",
    "                    agent2_feedback = agent2_claims.invoke({\"input\": text,  \"reinforcer_notes\": agent5_reinforcer})\n",
    "                    agent3_feedback = agent3_opinions.invoke({\"input\": text,  \"reinforcer_notes\": agent5_reinforcer})\n",
    "\n",
    "\n",
    "                # Judge\n",
    "                prompt_agent4 = ChatPromptTemplate.from_messages([\n",
    "                    (\"system\", \"You are tasked with judging whether the following claims are contained within the text.\"),\n",
    "                    (\"system\", \"Claims of facts are : {facts}\"),\n",
    "                    (\"system\", \"Verifiable claims are: {claims}\"),\n",
    "                    (\"system\", \"Un-veriable opinions are: {opinions}\"),\n",
    "                    (\"system\", \"These were judgements based on the following text: {input}\"),\n",
    "                ])\n",
    "\n",
    "                # Reinforcer\n",
    "                prompt_agent5 = ChatPromptTemplate.from_messages([\n",
    "                    (\"system\", \"Three agents have assessed a text for verifiable and unverifiable claims\"),\n",
    "                    (\"system\", \"A judge has compared these to the original text and provided the feedback: {judgement}\"),\n",
    "                    (\"user\", \"What changes should the agents implement?\"),\n",
    "                ])\n",
    "\n",
    "                agent4_judge = prompt_agent4 | llm | output_parser\n",
    "\n",
    "                agent4_feedback = agent4_judge.invoke(\n",
    "                    {\n",
    "                        \"facts\":agent1_feedback, \n",
    "                        \"verifclaims\": agent2_feedback, \n",
    "                        \"opinions\":agent3_feedback, \n",
    "                        \"input\": text\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if i < (n_loops):\n",
    "\n",
    "                    agent5_reinforcer = prompt_agent5 | llm | output_parser\n",
    "                    agent5_feedback = agent5_reinforcer.invoke(\n",
    "                        {\n",
    "                            \"judgement\":agent4_feedback\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            return agent4_feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23518cf",
   "metadata": {},
   "source": [
    "##### Define block of text\n",
    "- Source: New York Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5400995c-14a9-4e04-93b4-b37fac5f8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "The economic news in 2023 was almost miraculously good. Not only did Americaâ€™s economy defy widespread predictions of recession, it also defied claims that only a significant rise in unemployment could bring inflation under control. Instead, we got a combination of strong growth, unemployment near a 50-year low and plunging inflation.\n",
    "\n",
    "But last week, the Bureau of Labor Statistics reported that both the Consumer Price Index and the Producer Price Index rose 0.3 percent in January, more than most analysts expected. And the usual suspects â€” inflation perma-bears, political enemies of the Biden administration and economists who wrongly predicted that disinflation would require mass unemployment â€” jumped on the data as if it were a fumbled football.\n",
    "\n",
    "So, are the good times over?\n",
    "\n",
    "No. Everything we know suggests that those disappointing numbers were mostly a statistical blip rather than marking a significant worsening in inflation trends.\n",
    "\n",
    "Before I explain how such blips can happen, let me tell you what indicators I was looking at after the inflation reports.\n",
    "\n",
    "First, I was looking at financial markets, where instruments like inflation swaps and index bonds tell you what inflation rates investors putting real money on the line expect. The pricing on these instruments is still pointing to low inflation, around 2 percent or a bit more.\n",
    "\n",
    "Second, I was waiting to see what happened in the Atlanta Federal Reserveâ€™s survey of business inflation expectations, which asks businesses how much they expect costs to rise over the next year. If inflation were suddenly surging, youâ€™d expect businesses to notice. But their inflation expectations rose to 2.3 percent in February from â€¦ 2.2 percent in January.\n",
    "\n",
    "But if nothing much has changed, where did those slightly scary B.L.S. numbers come from?\n",
    "\n",
    "In principle, the government estimates overall consumer prices the same way the American Farm Bureau Federation estimates the price of a classic Thanksgiving dinner (which was, by the way, down 4.5 percent in 2023): it calculates the cost of buying a fixed basket of goods and services.\n",
    "\n",
    "In practice, our economy is a lot more complicated than a standardized holiday dinner menu, and estimating inflation involves a lot of fancy statistical footwork. The B.L.S. is extremely competent and professional â€” in fact, one rarely heralded policy advantage the United States has over other countries is that we generally have better data. But while I have nothing but praise for the bureau, its reports can still sometimes be misleading, for several reasons.\n",
    "\n",
    "One reason is that to make sense of monthly data, you need to adjust for seasonal factors. Some of these factors are obvious: fresh vegetables get more expensive in the winter, cheaper in the summer. Others are less obvious. Goldman Sachs, which correctly predicted a bump in official inflation, points out that there is a â€œJanuary effectâ€ on prices, because many companies raise their prices at the beginning of the year. And Goldman argued, in advance, that the official numbers wouldnâ€™t be sufficiently adjusted to reflect this effect, leading to a spurious bump in measured inflation â€” a bump that will vanish in the months ahead.\n",
    "\n",
    "Goldman also pointed out that the single largest component in the Consumer Price Index â€” 27 percent of the basket! â€” is a price nobody actually pays: ownersâ€™ equivalent rent, an estimate of what homeowners would be paying if they rented their houses. There are reasons the bureau measures housing costs this way, but there are also reasons to believe that in recent years that number has become misleading, distorting and exaggerating estimates of overall inflation. As it happens, the B.L.S. also produces an estimate of prices excluding ownersâ€™ equivalent rent, roughly matching the way European countries measure inflation. This â€œharmonizedâ€ index is up only 2.3 percent over the past year.\n",
    "\n",
    "If you find all of this a bit mind-numbing, let me tell you a secret â€” so do I, even though this is supposed to be my field. But the bottom line is important: Despite some disappointing numbers last week, the basic narrative hasnâ€™t changed. The U.S. economy continues to look like an amazing success story.\n",
    "\n",
    "Saying this leads, of course, to pushback from Republicans whoâ€™ve claimed ad nauseam that Bidenâ€™s â€œsocialistâ€ policies would be a disaster â€” and as I recently wrote, for such people believing is seeing, so they continue to insist that the economy is terrible even when by all objective measures, itâ€™s doing pretty well. You also get some pushback from people on the left, who apparently believe that a progressive president shouldnâ€™t be allowed to tout policy successes until he has completely eliminated poverty and insecurity â€” that is, never.\n",
    "\n",
    "The fact, however, is that Biden has put in place a very ambitious agenda â€” major enhancements of Obamacare, student debt relief, big infrastructure spending, large-scale promotion of semiconductors and green energy that have led to a surge in manufacturing investment. Many voices warned that he was overreaching, that the economy would pay a big price.\n",
    "\n",
    "But it hasnâ€™t. It turns out that we can, in fact, afford to do a lot to improve Americansâ€™ lives and invest in the future.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da970c09-6b09-4f14-abb8-4723cacd1b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to four agents\n",
      "\n",
      "System: Based on the provided text, the following are judged as claims, opinions, and unverifiable statements:\n",
      "\n",
      "Claims:\n",
      "\n",
      "1. The economic news in 2023 was almost miraculously good.\n",
      "2. Americaâ€™s economy defied widespread predictions of recession.\n",
      "3. Unemployment is near a 50-year low.\n",
      "4. Inflation is plunging.\n",
      "5. The Bureau of Labor Statistics reported that both the Consumer Price Index and the Producer Price Index rose 0.3 percent in January.\n",
      "\n",
      "Opinions:\n",
      "\n",
      "1. Biden's policies have been successful despite criticism from both Republicans and people on the left.\n",
      "\n",
      "Unverifiable statements:\n",
      "\n",
      "1. The article discusses the latest economic news in 2023 and how some numbers may be misleading.\n",
      "2. Many voices warned that Biden was overreaching, but the economy hasn't paid a big price.\n",
      "3. We can afford to do a lot to improve Americansâ€™ lives and invest in the future.\n"
     ]
    }
   ],
   "source": [
    "out = extract_verifiable_claims_via_debate(text, n_loops=1, n_agents=5)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "716a747a-c1fc-482a-8a64-71e613b93681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8411c80a",
   "metadata": {},
   "source": [
    "##### Define the Lang-graph implementation of the above workflow\n",
    "- Lang-graph allows for a faster, more flexible solution by defining nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18348efd-6675-4fdf-8d00-caff3793cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.graph import StateGraph, END\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_community.llms import Ollama\n",
    "# from typing import TypedDict, Annotated, Sequence\n",
    "# from langchain_core.messages import BaseMessage\n",
    "# from langchain.tools import Tool\n",
    "# from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "# import operator\n",
    "\n",
    "\n",
    "# llm = Ollama(model=\"llama2\")\n",
    "\n",
    "# # Define the output parser\n",
    "# output_parser = StrOutputParser()\n",
    "\n",
    "# fact_agent_messages = [\n",
    "#     (\"system\", \"You are tasked with extracting from text the claims of specific values, figures, or facts. Exclude any opinions or personal preferences.\"),\n",
    "#     (\"system\", \"You answer in short sentences.\"),\n",
    "#     (\"system\", \"The domain focus is politics.\"),\n",
    "#     (\"system\", \"Each reply is a bullet point.\"),\n",
    "#     (\"user\", \"The text is: {input}\")\n",
    "# ]\n",
    "\n",
    "# claim_agent_messages = [\n",
    "#     (\"system\", \"You are tasked with extracting from text the verifiable claims. If it is possible to verify, include it. Exclude any opinions or personal preferences.\"),\n",
    "#     (\"system\", \"You answer in short sentences.\"),\n",
    "#     (\"system\", \"The domain focus is politics.\"),\n",
    "#     (\"system\", \"Each reply is a bullet point.\"),\n",
    "#     (\"user\", \"The text is: {input}\")\n",
    "# ]\n",
    "\n",
    "# opinion_agent_messages = [\n",
    "#     (\"system\", \"You are tasked with extracting all opinions from the text which are not verifyable.\"),\n",
    "#     (\"system\", \"You answer in short sentences.\"),\n",
    "#     (\"system\", \"The domain focus is politics.\"),\n",
    "#     (\"system\", \"Each reply is a bullet point.\"),\n",
    "#     (\"user\", \"The text is: {input}\")\n",
    "# ]\n",
    "\n",
    "# formatter_agent_messages = [\n",
    "#     (\"system\", \"You are very organized and take inputs from different sources and make well formatted lists.\"),\n",
    "#     (\"system\", \"You take input from three sources, fact claims, opinions, and verifiable claims, and organize them. You do not add anything other than what is given to you.\"),\n",
    "#     (\"system\", \"You always output lists in the below format:\\n Fact Claims:\\n - fact claim 1\\n  - fact claim 2\\n\\n Verifiable Claims:\\n - verifiable claim 1\\n - verifiable claim 2\\n\\n Opinions:\\n - opinion 1\\n - opinion 2\"),\n",
    "#     (\"user\", \"Fact claims are:\\n {facts}\"),\n",
    "#     (\"user\", \"Verifiable claims are:\\n {claims}\"),\n",
    "#     (\"user\", \"Opinions are: \\n {opinions}\"),\n",
    "# ]\n",
    "\n",
    "# judge_agent_messages = [\n",
    "#     (\"system\", \"You are tasked with judging whether the following claims are contained within the text. You will be given a set of Fact Claims, Verifiable Claims, and Opinions from a text.\"),\n",
    "#     (\"system\", \"If these Claims, Facts, or Opinions are not in the original text, you will answer 'no'. If they are, answer 'yes'\"),\n",
    "#     (\"system\", \"You only respond with 'yes' or 'no', with no other text.\"),\n",
    "#     (\"system\", \"Claims, Facts, and Opinions:\\n{formatted_extractions}\\n\"),\n",
    "#     (\"system\", \"These were judgements based on the following text:\\n {input}\\n\"),\n",
    "#     (\"system\", \"Are the claims are contained within the text? Answer only 'yes' or 'no' with no other text.\")\n",
    "# ]\n",
    "\n",
    "# feedback_agent_messages = [\n",
    "#     (\"system\", \"Three agents have assessed a text for verifiable and unverifiable claims\"),\n",
    "#     (\"system\", \"A judge has compared these to the original text and decided that the extracted claims are not sufficient based on the original text.\"),\n",
    "#     (\"system\", \"The facts may not exist in the text, the opinions may not be opinions, or some other problem. Given the analysis and the original text, provide instructions to improve the output.\"),\n",
    "#     (\"user\", \"Extracted Facts, Claims, and Opinions:\\n {formatted_extractions}\"),\n",
    "#     (\"user\", \"Original Text:\\n {input}\"),\n",
    "#     (\"user\", \"What changes should the agents implement?\"),\n",
    "# ]\n",
    "\n",
    "\n",
    "# feedback_prompt = \"You have previously done this before, and a judge has suggested the following notes to improve your answer: {reinforcer_notes}\"\n",
    "\n",
    "\n",
    "# # facts_agent = fact_agent_messages | llm | output_parser\n",
    "# # claims_agent = claim_agent_messages | llm | output_parser\n",
    "# # opinions_agent = opinion_agent_messages | llm | output_parser\n",
    "# # judge_agent = judge_agent_messages | llm | output_parser\n",
    "\n",
    "# # Define the agent state\n",
    "# class AgentState(TypedDict):\n",
    "#     facts: str\n",
    "#     claims: str\n",
    "#     opinions: str\n",
    "#     formatted_extraction: str\n",
    "#     reinforcer_notes: str\n",
    "#     judge_output: str\n",
    "#     input_text: str\n",
    "#     iteration: int\n",
    "#     max_iterations: int\n",
    "#     final_output: str\n",
    "\n",
    "# def input(state):\n",
    "#     input_text = state[\"input_text\"]\n",
    "    \n",
    "#     return\n",
    "\n",
    "# def extract_facts(state):\n",
    "#     input_text = state[\"input_text\"]\n",
    "#     if state.get(\"reinforcer_notes\"):\n",
    "#         fact_agent_messages.append((\"system\", feedback_prompt))\n",
    "#     reinforcer_notes = state.get(\"reinforcer_notes\", \"\")\n",
    "    \n",
    "#     facts_prompt = ChatPromptTemplate.from_messages(fact_agent_messages)\n",
    "#     facts_agent = facts_prompt | llm | output_parser\n",
    "#     response = facts_agent.invoke({\"input\": input_text, \"reinforcer_notes\": reinforcer_notes})\n",
    "#     return {\"facts\": response}\n",
    "\n",
    "# def extract_opinions(state):\n",
    "#     input_text = state[\"input_text\"]\n",
    "#     if state.get(\"reinforcer_notes\"):\n",
    "#         opinions_agent_messages.append((\"system\", feedback_prompt))\n",
    "#     reinforcer_notes = state.get(\"reinforcer_notes\", \"\")\n",
    "\n",
    "#     opinions_prompt = ChatPromptTemplate.from_messages(opinion_agent_messages)\n",
    "#     opinions_agent = opinions_prompt | llm | output_parser\n",
    "#     response = opinions_agent.invoke({\"input\": input_text, \"reinforcer_notes\": reinforcer_notes})\n",
    "#     return {\"opinions\": response}\n",
    "\n",
    "# def extract_claims(state):\n",
    "#     input_text = state[\"input_text\"]\n",
    "#     if state.get(\"reinforcer_notes\"):\n",
    "#         claims_agent_messages.append((\"system\", feedback_prompt))\n",
    "#     reinforcer_notes = state.get(\"reinforcer_notes\", \"\")\n",
    "\n",
    "#     claims_prompt = ChatPromptTemplate.from_messages(claim_agent_messages)\n",
    "#     claims_agent = claims_prompt | llm | output_parser\n",
    "#     response = claims_agent.invoke({\"input\": input_text, \"reinforcer_notes\": reinforcer_notes})\n",
    "#     return {\"claims\": response}\n",
    "\n",
    "# def format_extractions(state):\n",
    "#     facts = state.get(\"facts\")\n",
    "#     opinions = state.get(\"opinions\")\n",
    "#     claims = state.get(\"claims\")\n",
    "#     formatter_prompt = ChatPromptTemplate.from_messages(formatter_agent_messages)\n",
    "#     formatter_agent = formatter_prompt | llm | output_parser\n",
    "#     formatted_text = formatter_agent.invoke({\"facts\": facts, \"claims\": claims, \"opinions\": opinions})\n",
    "#     return {'formatted_extraction': formatted_text}\n",
    "\n",
    "# def extract_judgement(state):\n",
    "#     formatted_extraction = state.get(\"formatted_extraction\")\n",
    "#     input = state.get(\"input_text\")\n",
    "\n",
    "#     judge_prompt = ChatPromptTemplate.from_messages(judge_agent_messages)\n",
    "#     judge_agent = judge_prompt | llm | output_parser\n",
    "#     verdict = judge_agent.invoke({\"input\": input, \"formatted_extractions\":formatted_extraction})\n",
    "#     return {\"judge_output\": verdict}\n",
    "\n",
    "# def feedback_on_incorrect_extractions(state):\n",
    "#     formatted_extraction = state.get(\"formatted_extraction\")\n",
    "#     input = state.get(\"input_text\")\n",
    "#     instructor_prompt = ChatPromptTemplate.from_messages(feedback_agent_messages)\n",
    "#     instructor_agent = instructor_prompt | llm | output_parser\n",
    "#     instructions = instructor_agent.invoke({\"input\": input, \"formatted_extractions\":formatted_extraction})\n",
    "#     return {\"reinforcer_notes\": instructions}\n",
    "\n",
    "# # def google_search_agent(stuff):\n",
    "# #     search = GoogleSearchAPIWrapper()\n",
    "# #     os.environ[\"GOOGLE_CSE_ID\"] = \"\"\n",
    "# #     os.environ[\"GOOGLE_API_KEY\"] = \"\"    \n",
    "# #     tool = Tool(\n",
    "# #         name=\"google_search\",\n",
    "# #         description=\"Search Google for recent results.\",\n",
    "# #         func=search.run,\n",
    "# #     )\n",
    "\n",
    "# #     searcher_prompt = ChatPromptTemplate.from_messages((\"system\",\"Please search the following facts:\\n{stuff}\"))\n",
    "# #     searcher_agent = searcher_prompt | llm | output_parser | tool\n",
    "# #     res = searcher_agent.invoke({\"stuff\":stuff})\n",
    "# #     return res\n",
    "                                                        \n",
    "\n",
    "# def judge_happy(state):\n",
    "#     judges_verdict = state.get(\"judge_output\")\n",
    "#     judges_verdict = judges_verdict.strip().replace(\"\\n\", \"\").lower()\n",
    "#     print(judges_verdict)\n",
    "#     if judges_verdict == 'yes':\n",
    "#         return 'continue'\n",
    "#     return 'feedback'\n",
    "\n",
    "# def print_output(state):\n",
    "#     return {\"final_output\": \"HAPPY\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7263e9a-aac7-489e-b3d8-4c163d6c402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = StateGraph(AgentState)\n",
    "\n",
    "# # Define nodes in our graph\n",
    "# graph.add_node(\"facts_agent\", extract_facts)\n",
    "# graph.add_node(\"opinions_agent\", extract_opinions)\n",
    "# graph.add_node(\"claims_agent\", extract_claims)\n",
    "# graph.add_node(\"formatting_agent\", format_extractions)\n",
    "# graph.add_node(\"judge_agent\", extract_judgement)\n",
    "# graph.add_node(\"feedback_agent\", feedback_on_incorrect_extractions)\n",
    "# graph.add_node(\"output\", print_output)\n",
    "\n",
    "\n",
    "# graph.add_edge('facts_agent', 'opinions_agent')\n",
    "# graph.add_edge('opinions_agent', 'claims_agent')\n",
    "# graph.add_edge('claims_agent', 'formatting_agent')\n",
    "# graph.add_edge('formatting_agent', 'judge_agent')\n",
    "# graph.add_conditional_edges(\n",
    "#     \"judge_agent\",\n",
    "#     judge_happy,\n",
    "#     {\n",
    "#         \"feedback\": \"feedback_agent\",\n",
    "#         \"continue\": \"output\"\n",
    "#     }\n",
    "# )\n",
    "# graph.add_edge('feedback_agent', 'facts_agent')\n",
    "# graph.add_edge('output', END)\n",
    "\n",
    "# graph.set_entry_point(\"facts_agent\")\n",
    "\n",
    "# verify = graph.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6ab0cc",
   "metadata": {},
   "source": [
    "##### Run the Lang-graph implementation on a body of text\n",
    "- This prints the output from each of the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5efa0143-4ee8-40f4-a90d-79c2f2cf5d36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "Fact Claims:\n",
      "\n",
      "* The economic news in 2023 was mostly good, with strong growth, low unemployment, and falling inflation.\n",
      "* Despite some disappointing numbers last week, the basic narrative hasn't changed: the US economy is doing well.\n",
      "* Investors are still expecting low inflation, around 2-3%, based on financial market indicators.\n",
      "* Businesses' inflation expectations also rose to 2.3% in February, indicating that inflation is not suddenly surging.\n",
      "* The BLS numbers can be misleading due to seasonal factors and the \"January effect\" on prices.\n",
      "* Excluding owners' equivalent rent, the \"harmonized\" index is up only 2.3% over the past year.\n",
      "\n",
      "Verifiable Claims:\n",
      "\n",
      "* The economic news in 2023 was good, with strong growth, low unemployment, and plunging inflation.\n",
      "* The Bureau of Labor Statistics reported that both the Consumer Price Index and the Producer Price Index rose 0.3 percent in January, more than expected.\n",
      "* Despite some disappointing inflation numbers last week, the basic narrative hasn't changed, and the economy continues to look like an amazing success story.\n",
      "\n",
      "Opinions:\n",
      "\n",
      "* The article states that some inflation numbers were unexpectedly high last week, but the author believes this is just a statistical blip rather than a significant change in inflation trends.\n",
      "* The author points out that financial markets and business inflation expectations still indicate low inflation, around 2-3%.\n",
      "* The author suggests that the slightly higher inflation numbers may be due to seasonal factors or measurement issues, such as the \"January effect\" on prices or the misleading nature of owners' equivalent rent.\n"
     ]
    }
   ],
   "source": [
    "# inputs = {\"input_text\": text}\n",
    "\n",
    "# out = None\n",
    "# for output in verify.stream(inputs):\n",
    "#     # stream() yields dictionaries with output keyed by node name\n",
    "#     for key, value in output.items():\n",
    "#         if key == '__end__':\n",
    "#         # print(f\"Output from node '{key}':\")\n",
    "#         # print(\"---\")\n",
    "#             out = value['formatted_extraction']\n",
    "#             print(out)\n",
    "#     # print(\"\\n---\\n\")\n",
    "\n",
    "# # for output in verify.stream(inputs):\n",
    "# #     print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0413e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_claims(text):\n",
    "\n",
    "    from langgraph.graph import StateGraph, END\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_community.llms import Ollama\n",
    "    from typing import TypedDict, Annotated, Sequence\n",
    "    from langchain_core.messages import BaseMessage\n",
    "    from langchain.tools import Tool\n",
    "    from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "    import operator\n",
    "\n",
    "\n",
    "    llm = Ollama(model=\"llama2\")\n",
    "\n",
    "    # Define the output parser\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    fact_agent_messages = [\n",
    "        (\"system\", \"You are tasked with extracting from text the claims of specific values, figures, or facts. Exclude any opinions or personal preferences.\"),\n",
    "        (\"system\", \"You answer in short sentences.\"),\n",
    "        (\"system\", \"The domain focus is politics.\"),\n",
    "        (\"system\", \"Each reply is a bullet point.\"),\n",
    "        (\"user\", \"The text is: {input}\")\n",
    "    ]\n",
    "\n",
    "    claim_agent_messages = [\n",
    "        (\"system\", \"You are tasked with extracting from text the verifiable claims. If it is possible to verify, include it. Exclude any opinions or personal preferences.\"),\n",
    "        (\"system\", \"You answer in short sentences.\"),\n",
    "        (\"system\", \"The domain focus is politics.\"),\n",
    "        (\"system\", \"Each reply is a bullet point.\"),\n",
    "        (\"user\", \"The text is: {input}\")\n",
    "    ]\n",
    "\n",
    "    opinion_agent_messages = [\n",
    "        (\"system\", \"You are tasked with extracting all opinions from the text which are not verifyable.\"),\n",
    "        (\"system\", \"You answer in short sentences.\"),\n",
    "        (\"system\", \"The domain focus is politics.\"),\n",
    "        (\"system\", \"Each reply is a bullet point.\"),\n",
    "        (\"user\", \"The text is: {input}\")\n",
    "    ]\n",
    "\n",
    "    formatter_agent_messages = [\n",
    "        (\"system\", \"You are very organized and take inputs from different sources and make well formatted lists.\"),\n",
    "        (\"system\", \"You take input from three sources, fact claims, opinions, and verifiable claims, and organize them. You do not add anything other than what is given to you.\"),\n",
    "        (\"system\", \"You always output lists in the below format:\\n Fact Claims:\\n - fact claim 1\\n  - fact claim 2\\n\\n Verifiable Claims:\\n - verifiable claim 1\\n - verifiable claim 2\\n\\n Opinions:\\n - opinion 1\\n - opinion 2\"),\n",
    "        (\"user\", \"Fact claims are:\\n {facts}\"),\n",
    "        (\"user\", \"Verifiable claims are:\\n {claims}\"),\n",
    "        (\"user\", \"Opinions are: \\n {opinions}\"),\n",
    "    ]\n",
    "\n",
    "    judge_agent_messages = [\n",
    "        (\"system\", \"You are tasked with judging whether the following claims are contained within the text. You will be given a set of Fact Claims, Verifiable Claims, and Opinions from a text.\"),\n",
    "        (\"system\", \"If these Claims, Facts, or Opinions are not in the original text, you will answer 'no'. If they are, answer 'yes'\"),\n",
    "        (\"system\", \"You only respond with 'yes' or 'no', with no other text.\"),\n",
    "        (\"system\", \"Claims, Facts, and Opinions:\\n{formatted_extractions}\\n\"),\n",
    "        (\"system\", \"These were judgements based on the following text:\\n {input}\\n\"),\n",
    "        (\"system\", \"Are the claims are contained within the text? Answer only 'yes' or 'no' with no other text.\")\n",
    "    ]\n",
    "\n",
    "    feedback_agent_messages = [\n",
    "        (\"system\", \"Three agents have assessed a text for verifiable and unverifiable claims\"),\n",
    "        (\"system\", \"A judge has compared these to the original text and decided that the extracted claims are not sufficient based on the original text.\"),\n",
    "        (\"system\", \"The facts may not exist in the text, the opinions may not be opinions, or some other problem. Given the analysis and the original text, provide instructions to improve the output.\"),\n",
    "        (\"user\", \"Extracted Facts, Claims, and Opinions:\\n {formatted_extractions}\"),\n",
    "        (\"user\", \"Original Text:\\n {input}\"),\n",
    "        (\"user\", \"What changes should the agents implement?\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "    feedback_prompt = \"You have previously done this before, and a judge has suggested the following notes to improve your answer: {reinforcer_notes}\"\n",
    "\n",
    "\n",
    "    # facts_agent = fact_agent_messages | llm | output_parser\n",
    "    # claims_agent = claim_agent_messages | llm | output_parser\n",
    "    # opinions_agent = opinion_agent_messages | llm | output_parser\n",
    "    # judge_agent = judge_agent_messages | llm | output_parser\n",
    "\n",
    "    # Define the agent state\n",
    "    class AgentState(TypedDict):\n",
    "        facts: str\n",
    "        claims: str\n",
    "        opinions: str\n",
    "        formatted_extraction: str\n",
    "        reinforcer_notes: str\n",
    "        judge_output: str\n",
    "        input_text: str\n",
    "        iteration: int\n",
    "        max_iterations: int\n",
    "        final_output: str\n",
    "\n",
    "    def input(state):\n",
    "        input_text = state[\"input_text\"]\n",
    "        \n",
    "        return\n",
    "\n",
    "    def extract_facts(state):\n",
    "        input_text = state[\"input_text\"]\n",
    "        if state.get(\"reinforcer_notes\"):\n",
    "            fact_agent_messages.append((\"system\", feedback_prompt))\n",
    "        reinforcer_notes = state.get(\"reinforcer_notes\", \"\")\n",
    "        \n",
    "        facts_prompt = ChatPromptTemplate.from_messages(fact_agent_messages)\n",
    "        facts_agent = facts_prompt | llm | output_parser\n",
    "        response = facts_agent.invoke({\"input\": input_text, \"reinforcer_notes\": reinforcer_notes})\n",
    "        return {\"facts\": response}\n",
    "\n",
    "    def extract_opinions(state):\n",
    "        input_text = state[\"input_text\"]\n",
    "        if state.get(\"reinforcer_notes\"):\n",
    "            opinions_agent_messages.append((\"system\", feedback_prompt))\n",
    "        reinforcer_notes = state.get(\"reinforcer_notes\", \"\")\n",
    "\n",
    "        opinions_prompt = ChatPromptTemplate.from_messages(opinion_agent_messages)\n",
    "        opinions_agent = opinions_prompt | llm | output_parser\n",
    "        response = opinions_agent.invoke({\"input\": input_text, \"reinforcer_notes\": reinforcer_notes})\n",
    "        return {\"opinions\": response}\n",
    "\n",
    "    def extract_claims(state):\n",
    "        input_text = state[\"input_text\"]\n",
    "        if state.get(\"reinforcer_notes\"):\n",
    "            claims_agent_messages.append((\"system\", feedback_prompt))\n",
    "        reinforcer_notes = state.get(\"reinforcer_notes\", \"\")\n",
    "\n",
    "        claims_prompt = ChatPromptTemplate.from_messages(claim_agent_messages)\n",
    "        claims_agent = claims_prompt | llm | output_parser\n",
    "        response = claims_agent.invoke({\"input\": input_text, \"reinforcer_notes\": reinforcer_notes})\n",
    "        return {\"claims\": response}\n",
    "\n",
    "    def format_extractions(state):\n",
    "        facts = state.get(\"facts\")\n",
    "        opinions = state.get(\"opinions\")\n",
    "        claims = state.get(\"claims\")\n",
    "        formatter_prompt = ChatPromptTemplate.from_messages(formatter_agent_messages)\n",
    "        formatter_agent = formatter_prompt | llm | output_parser\n",
    "        formatted_text = formatter_agent.invoke({\"facts\": facts, \"claims\": claims, \"opinions\": opinions})\n",
    "        return {'formatted_extraction': formatted_text}\n",
    "\n",
    "    def extract_judgement(state):\n",
    "        formatted_extraction = state.get(\"formatted_extraction\")\n",
    "        input = state.get(\"input_text\")\n",
    "\n",
    "        judge_prompt = ChatPromptTemplate.from_messages(judge_agent_messages)\n",
    "        judge_agent = judge_prompt | llm | output_parser\n",
    "        verdict = judge_agent.invoke({\"input\": input, \"formatted_extractions\":formatted_extraction})\n",
    "        return {\"judge_output\": verdict}\n",
    "\n",
    "    def feedback_on_incorrect_extractions(state):\n",
    "        formatted_extraction = state.get(\"formatted_extraction\")\n",
    "        input = state.get(\"input_text\")\n",
    "        instructor_prompt = ChatPromptTemplate.from_messages(feedback_agent_messages)\n",
    "        instructor_agent = instructor_prompt | llm | output_parser\n",
    "        instructions = instructor_agent.invoke({\"input\": input, \"formatted_extractions\":formatted_extraction})\n",
    "        return {\"reinforcer_notes\": instructions}\n",
    "\n",
    "    # def google_search_agent(stuff):\n",
    "    #     search = GoogleSearchAPIWrapper()\n",
    "    #     os.environ[\"GOOGLE_CSE_ID\"] = \"\"\n",
    "    #     os.environ[\"GOOGLE_API_KEY\"] = \"\"    \n",
    "    #     tool = Tool(\n",
    "    #         name=\"google_search\",\n",
    "    #         description=\"Search Google for recent results.\",\n",
    "    #         func=search.run,\n",
    "    #     )\n",
    "\n",
    "    #     searcher_prompt = ChatPromptTemplate.from_messages((\"system\",\"Please search the following facts:\\n{stuff}\"))\n",
    "    #     searcher_agent = searcher_prompt | llm | output_parser | tool\n",
    "    #     res = searcher_agent.invoke({\"stuff\":stuff})\n",
    "    #     return res\n",
    "                                                            \n",
    "\n",
    "    def judge_happy(state):\n",
    "        judges_verdict = state.get(\"judge_output\")\n",
    "        judges_verdict = judges_verdict.strip().replace(\"\\n\", \"\").lower()\n",
    "        print(judges_verdict)\n",
    "        if judges_verdict == 'yes':\n",
    "            return 'continue'\n",
    "        return 'feedback'\n",
    "\n",
    "    def print_output(state):\n",
    "        return {\"final_output\": \"HAPPY\"}\n",
    "    \n",
    "    graph = StateGraph(AgentState)\n",
    "\n",
    "    # Define nodes in our graph\n",
    "    graph.add_node(\"facts_agent\", extract_facts)\n",
    "    graph.add_node(\"opinions_agent\", extract_opinions)\n",
    "    graph.add_node(\"claims_agent\", extract_claims)\n",
    "    graph.add_node(\"formatting_agent\", format_extractions)\n",
    "    graph.add_node(\"judge_agent\", extract_judgement)\n",
    "    graph.add_node(\"feedback_agent\", feedback_on_incorrect_extractions)\n",
    "    graph.add_node(\"output\", print_output)\n",
    "\n",
    "\n",
    "    graph.add_edge('facts_agent', 'opinions_agent')\n",
    "    graph.add_edge('opinions_agent', 'claims_agent')\n",
    "    graph.add_edge('claims_agent', 'formatting_agent')\n",
    "    graph.add_edge('formatting_agent', 'judge_agent')\n",
    "    graph.add_conditional_edges(\n",
    "        \"judge_agent\",\n",
    "        judge_happy,\n",
    "        {\n",
    "            \"feedback\": \"feedback_agent\",\n",
    "            \"continue\": \"output\"\n",
    "        }\n",
    "    )\n",
    "    graph.add_edge('feedback_agent', 'facts_agent')\n",
    "    graph.add_edge('output', END)\n",
    "\n",
    "    graph.set_entry_point(\"facts_agent\")\n",
    "\n",
    "    verify = graph.compile()\n",
    "\n",
    "    inputs = {\"input_text\": text}\n",
    "\n",
    "    out = None\n",
    "    for output in verify.stream(inputs):\n",
    "        # stream() yields dictionaries with output keyed by node name\n",
    "        for key, value in output.items():\n",
    "            if key == '__end__':\n",
    "            # print(f\"Output from node '{key}':\")\n",
    "            # print(\"---\")\n",
    "                out = value['formatted_extraction']\n",
    "                print(out)\n",
    "        # print(\"\\n---\\n\")\n",
    "\n",
    "    # for output in verify.stream(inputs):\n",
    "    #     print(output)\n",
    "                \n",
    "    return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dfd3c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_judge(text):\n",
    "\n",
    "    from langchain_community.llms import Ollama\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "    # Get the LLM from Ollama\n",
    "    llm = Ollama(model=\"llama2\")\n",
    "\n",
    "    # Define the output parser\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are tasked with extracting verifiable claims in a text.\"),\n",
    "        (\"system\", \"Answer with only a yes or a no.\"),\n",
    "        (\"system\", \"Do you believe the following is a verifiable claim that can be fact checked?\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    # Define the output parser\n",
    "    output_parser = StrOutputParser()\n",
    "\n",
    "    # Define the chain\n",
    "    chain = prompt | llm | output_parser\n",
    "\n",
    "    return str(chain.invoke({\"input\": text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9c2e7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m temp \u001b[38;5;241m=\u001b[39m extract_claims(\u001b[43mtext\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "temp = extract_claims(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c14af18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact Claims:\n",
      "\n",
      "* The Bureau of Labor Statistics reported a 0.3% increase in both the Consumer Price Index and the Producer Price Index in January, higher than expected by most analysts.\n",
      "* Despite this, the overall narrative remains unchanged, with the economy continuing to perform well according to objective measures.\n",
      "* The reasons for the slight increase in inflation are complex and involve factors such as seasonal adjustments, the \"January effect\" on prices, and the misleading nature of owners' equivalent rent.\n",
      "* Expectations for inflation remain low, with financial markets still pricing in low inflation rates around 2-3%.\n",
      "* Businesses' inflation expectations also remained stable at 2.3% in February, suggesting that there has been no significant change in inflation trends.\n",
      "\n",
      "Verifiable Claims:\n",
      "\n",
      "* The US economy is doing well, with strong growth, low unemployment, and falling inflation.\n",
      "* Despite some disappointing numbers last week, the basic narrative hasn't changed - the US economy continues to look like an amazing success story.\n",
      "* Investors putting real money on the line still expect low inflation, around 2-3%.\n",
      "* Businesses' inflation expectations rose to 2.3% in February from 2.2% in January.\n",
      "* The BLS is extremely competent and professional, but its reports can sometimes be misleading due to factors such as seasonal adjustments and the \"January effect\" on prices.\n",
      "* The single largest component in the Consumer Price Index (CPI) - owners' equivalent rent - has become misleading, distorting and exaggerating estimates of overall inflation.\n",
      "* The \"harmonized\" index, which excludes owners' equivalent rent, is up only 2.3% over the past year.\n",
      "\n",
      "Opinions:\n",
      "\n",
      "* The article states that the Bureau of Labor Statistics (BLS) reported an increase in inflation, but this is seen as a statistical blip rather than a significant worsening of inflation trends.\n",
      "* Some people, including Republicans and those on the left, have been critical of President Biden's policies, predicting negative outcomes, but the article states that these predictions have been unfounded.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fact</td>\n",
       "      <td>The Bureau of Labor Statistics reported a 0.3%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fact</td>\n",
       "      <td>Despite this, the overall narrative remains un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fact</td>\n",
       "      <td>The reasons for the slight increase in inflati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fact</td>\n",
       "      <td>Expectations for inflation remain low, with fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fact</td>\n",
       "      <td>Businesses' inflation expectations also remain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Verifiable</td>\n",
       "      <td>The US economy is doing well, with strong grow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Verifiable</td>\n",
       "      <td>Despite some disappointing numbers last week, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Verifiable</td>\n",
       "      <td>Investors putting real money on the line still...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Verifiable</td>\n",
       "      <td>Businesses' inflation expectations rose to 2.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Verifiable</td>\n",
       "      <td>The BLS is extremely competent and professiona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Verifiable</td>\n",
       "      <td>The single largest component in the Consumer P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Verifiable</td>\n",
       "      <td>The \"harmonized\" index, which excludes owners'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unverifiable</td>\n",
       "      <td>The article states that the Bureau of Labor St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unverifiable</td>\n",
       "      <td>Some people, including Republicans and those o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Category                                               Text\n",
       "0          Fact  The Bureau of Labor Statistics reported a 0.3%...\n",
       "0          Fact  Despite this, the overall narrative remains un...\n",
       "0          Fact  The reasons for the slight increase in inflati...\n",
       "0          Fact  Expectations for inflation remain low, with fi...\n",
       "0          Fact  Businesses' inflation expectations also remain...\n",
       "0    Verifiable  The US economy is doing well, with strong grow...\n",
       "0    Verifiable  Despite some disappointing numbers last week, ...\n",
       "0    Verifiable  Investors putting real money on the line still...\n",
       "0    Verifiable  Businesses' inflation expectations rose to 2.3...\n",
       "0    Verifiable  The BLS is extremely competent and professiona...\n",
       "0    Verifiable  The single largest component in the Consumer P...\n",
       "0    Verifiable  The \"harmonized\" index, which excludes owners'...\n",
       "0  Unverifiable  The article states that the Bureau of Labor St...\n",
       "0  Unverifiable  Some people, including Republicans and those o..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datas = []\n",
    "\n",
    "facts = False\n",
    "vclaims = False\n",
    "uclaims = False\n",
    "\n",
    "for i in temp.split('\\n'):\n",
    "    print(str(i))\n",
    "    if str(i) != '':\n",
    "\n",
    "        if str(i).lower().replace(':','').replace('-','') in ['fact claims', 'facts', 'factual claims', 'factual statements', 'fact', 'statements of fact']:\n",
    "            facts = True\n",
    "            vclaims = False\n",
    "            uclaims = False\n",
    "            continue\n",
    "\n",
    "        if str(i).lower().replace(':','').replace('-','') in ['verifiable claims', 'verifiable', 'claims']:\n",
    "            vclaims = True\n",
    "            facts = False\n",
    "            uclaims = False\n",
    "            continue\n",
    "\n",
    "        if str(i).lower().replace(':','').replace('-','') in ['opinions', 'unverifiable claims', 'unverified', 'not verifiable']:\n",
    "            uclaims = True\n",
    "            facts = False\n",
    "            vclaims = False\n",
    "            continue\n",
    "\n",
    "        if facts:\n",
    "            temp_data = datas.append(pd.DataFrame(data={'Category': 'Fact', 'Text': str(i).replace('* ','')}, index=[0]))\n",
    "\n",
    "        if vclaims:\n",
    "            temp_data = datas.append(pd.DataFrame(data={'Category': 'Verifiable', 'Text': str(i).replace('* ','')}, index=[0])) \n",
    "        \n",
    "        if uclaims:\n",
    "            temp_data = datas.append(pd.DataFrame(data={'Category': 'Unverifiable', 'Text': str(i).replace('* ','')}, index=[0]))\n",
    "            \n",
    "data = pd.concat(datas)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7399bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_claim_judge_pipeline(text, one_claim=False):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    # Check whether the input is just one sentence:\n",
    "    if (len(str(text).split('.')) >= 1) and not one_claim:\n",
    "        agent_response = extract_claims(text)\n",
    "\n",
    "        datas = []\n",
    "\n",
    "        facts = False\n",
    "        vclaims = False\n",
    "        uclaims = False\n",
    "\n",
    "        for i in temp.split('\\n'):\n",
    "            print(str(i))\n",
    "            if str(i) != '':\n",
    "\n",
    "                if str(i).lower().replace(':','').replace('-','') in ['fact claims', 'facts', 'factual claims', 'factual statements', 'fact', 'statements of fact']:\n",
    "                    facts = True\n",
    "                    vclaims = False\n",
    "                    uclaims = False\n",
    "                    continue\n",
    "\n",
    "                if str(i).lower().replace(':','').replace('-','') in ['verifiable claims', 'verifiable', 'claims']:\n",
    "                    vclaims = True\n",
    "                    facts = False\n",
    "                    uclaims = False\n",
    "                    continue\n",
    "\n",
    "                if str(i).lower().replace(':','').replace('-','') in ['opinions', 'unverifiable claims', 'unverified', 'not verifiable']:\n",
    "                    uclaims = True\n",
    "                    facts = False\n",
    "                    vclaims = False\n",
    "                    continue\n",
    "\n",
    "                if facts:\n",
    "                    temp_data = datas.append(pd.DataFrame(data={'Category': 'Fact', 'Text': str(i).replace('* ','')}, index=[0]))\n",
    "\n",
    "                if vclaims:\n",
    "                    temp_data = datas.append(pd.DataFrame(data={'Category': 'Verifiable', 'Text': str(i).replace('* ','')}, index=[0])) \n",
    "                \n",
    "                if uclaims:\n",
    "                    temp_data = datas.append(pd.DataFrame(data={'Category': 'Unverifiable', 'Text': str(i).replace('* ','')}, index=[0]))\n",
    "                    \n",
    "        data = pd.concat(datas)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        # We just have one sentence that can directly be fed into the google search engine\n",
    "\n",
    "        verifiable = simple_judge(text)\n",
    "\n",
    "        if 'yes' in verifiable or 'sure' in verifiable:\n",
    "\n",
    "            # We have determined it to be verifiable\n",
    "            data = pd.DataFrame(data={'Category': 'Verifiable', 'Text': str(text).replace('* ','')}, index=[0])\n",
    "\n",
    "        else:\n",
    "            data = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    if not data.empty:\n",
    "\n",
    "        responses = []\n",
    "        for i, row in data[data['Category'].isin(['Verifiable','Fact'])].iterrows():\n",
    "\n",
    "            responses.append(google_search_agent(stuff=str(row['Text']), response_option='minimal', model_option='rag', return_information=False))\n",
    "\n",
    "        return responses\n",
    "\n",
    "    else:\n",
    "        return ['No Verifiable Claims']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e581e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer, timetaken = google_search_agent('The economic news in 2023 was good, with strong growth, low unemployment, and plunging inflation.', response_option='minimal')\n",
    "# print(answer)\n",
    "# print('It took:',timetaken,'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "451f20d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer, timetaken = google_search_agent('The Bureau of Labor Statistics reported that both the Consumer Price Index and the Producer Price Index rose 0.3 percent in January, more than expected.', response_option='minimal')\n",
    "# print(answer)\n",
    "# print('It took:',timetaken,'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3620f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer, timetaken = google_search_agent(\"Despite some disappointing inflation numbers last week, the basic narrative hasn't changed, and the economy continues to look like an amazing success story.\", response_option='minimal')\n",
    "# print(answer)\n",
    "# print('It took:',timetaken,'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0145a58",
   "metadata": {},
   "source": [
    "##### Connect the verifiable claims to the search API and RAG for final decision-making\n",
    "- This requires the user to define a custom search in Google and set up the API connection\n",
    "- Note: Google allows only 100 searches a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98905f5e-03e5-41d8-be40-4533017f7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "def google_search_agent(stuff, response_option=None, model_option='rag', return_information=False):\n",
    "    \n",
    "    try:\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "\n",
    "        if model_option == 'baseline':\n",
    "\n",
    "                search_prompt = ChatPromptTemplate.from_messages([\n",
    "                    (\"system\",\"you, please answer the question.\\nQUESTION\\n{question}\"),\n",
    "                    (\"system\",\"Answer will be false or true.\"),\n",
    "                    (\"system\",\"Keep verbosity low.\"),\n",
    "                    (\"system\",\"Answer in one word with your judgement, then provide a source and justification.\"),\n",
    "                    (\"system\",\"The source should include an internet link.\")\n",
    "                    ])\n",
    "                \n",
    "                search_agent = search_prompt | llm | output_parser\n",
    "                res = search_agent.invoke({\"question\":stuff})\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "            search = GoogleSearchAPIWrapper()\n",
    "                \n",
    "            tool = Tool(\n",
    "                name=\"google_search\",\n",
    "                description=\"Search Google for recent results.\",\n",
    "                func=search.run,\n",
    "            )\n",
    "\n",
    "            if response_option == 'minimal':\n",
    "                search_prompt = ChatPromptTemplate.from_messages([\n",
    "                    (\"system\",\"you, please answer the question.\\nINFO:\\n{information}\\nQUESTION\\n{question}\"),\n",
    "                    (\"system\",\"Answer will be false or true.\"),\n",
    "                    (\"system\",\"Keep verbosity low.\"),\n",
    "                    (\"system\",\"Answer in one word with your judgement, then provide a source and justification.\"),\n",
    "                    (\"system\",\"The source should include an internet link.\")\n",
    "                    ])\n",
    "            else:\n",
    "                search_prompt = ChatPromptTemplate.from_messages((\"system\",\"you, please answer the question.\\nINFO:\\n{information}\\nQUESTION\\n{question}\"))\n",
    "            \n",
    "            info = tool.invoke(stuff)\n",
    "            \n",
    "            # print(info)\n",
    "            search_agent = search_prompt | llm | output_parser\n",
    "            res = search_agent.invoke({\"question\":stuff, \"information\":info})\n",
    "            \n",
    "        time_taken = time.time() - start_time\n",
    "\n",
    "        if return_information:\n",
    "            return res, time_taken, '<'+str(info)+'>', tool\n",
    "        else:\n",
    "            return res, time_taken, '-', '-'\n",
    "    except Exception as e:\n",
    "         print(e)\n",
    "         return None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be8d194c-12bb-4a04-8629-cf9c62484ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.tools import Tool\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "import os\n",
    "\n",
    "# INSERT API KEYS HERE\n",
    "# Search API versions for Google: \n",
    "# - 0 = Entire web (v0)\n",
    "# - 1 = politifact plus dictionaries (v1)\n",
    "# - 2 = Just dictionaries (v2)\n",
    "# - 3 = News sources (new york times, la times, washington post, economist, hbr)\n",
    "\n",
    "search_version = 0\n",
    "\n",
    "search_dict_for_keys = {\n",
    "    0: \"\",\n",
    "    1: \"\",\n",
    "    2: \"\",\n",
    "    3: \"\",\n",
    "}\n",
    "\n",
    "if search_version not in list(search_dict_for_keys.keys()):\n",
    "    raise Exception('API search version not supported. Try one of 0,1,2,3.')\n",
    "\n",
    "\n",
    "# Set API key and CSE ID\n",
    "os.environ[\"GOOGLE_CSE_ID\"] = search_dict_for_keys[search_version]\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
    "\n",
    "out = google_search_agent(\"Biden is 900 years old.\", response_option='minimal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b5ab391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 4.766409873962402 seconds\n"
     ]
    }
   ],
   "source": [
    "print('Took',out[1],'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57bbd359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: False\n",
      "\n",
      "Justification: Based on the information provided in the text, President Biden is not 900 years old. In fact, he is 80 years old, as stated in the passage. (Source: Mar 21, 2023 ... Biden is 80 years old and walks stiffly due to degenerative osteoarthriticÂ ...)\n"
     ]
    }
   ],
   "source": [
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89566022-6769-484f-99d2-f6bae944c8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Tool in module langchain_core.tools object:\n",
      "\n",
      "class Tool(BaseTool)\n",
      " |  Tool(name: 'str', func: 'Optional[Callable]', description: 'str', *, args_schema: Optional[Type[pydantic.v1.main.BaseModel]] = None, return_direct: bool = False, verbose: bool = False, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, handle_tool_error: Union[bool, str, Callable[[langchain_core.tools.ToolException], str], NoneType] = False, handle_validation_error: Union[bool, str, Callable[[pydantic.v1.error_wrappers.ValidationError], str], NoneType] = False, coroutine: Optional[Callable[..., Awaitable[str]]] = None) -> None\n",
      " |\n",
      " |  Tool that takes in function or coroutine directly.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Tool\n",
      " |      BaseTool\n",
      " |      langchain_core.runnables.base.RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.v1.main.BaseModel\n",
      " |      pydantic.v1.utils.Representation\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, name: 'str', func: 'Optional[Callable]', description: 'str', **kwargs: 'Any') -> 'None'\n",
      " |      Initialize tool.\n",
      " |\n",
      " |  async ainvoke(self, input: 'Union[str, Dict]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Any'\n",
      " |      Default implementation of ainvoke, calls invoke from a thread.\n",
      " |\n",
      " |      The default implementation allows usage of async code even if\n",
      " |      the runnable did not implement a native async version of invoke.\n",
      " |\n",
      " |      Subclasses should override this method if they can run asynchronously.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  from_function(func: 'Optional[Callable]', name: 'str', description: 'str', return_direct: 'bool' = False, args_schema: 'Optional[Type[BaseModel]]' = None, coroutine: 'Optional[Callable[..., Awaitable[Any]]]' = None, **kwargs: 'Any') -> 'Tool' from pydantic.v1.main.ModelMetaclass\n",
      " |      Initialize tool from a function.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  args\n",
      " |      The tool's input arguments.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'coroutine': 'Optional[Callable[..., Awaitable[str]...\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
      " |\n",
      " |  __custom_root_type__ = False\n",
      " |\n",
      " |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True}\n",
      " |\n",
      " |  __fields__ = {'args_schema': ModelField(name='args_schema', type=Optio...\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __include_fields__ = None\n",
      " |\n",
      " |  __parameters__ = ()\n",
      " |\n",
      " |  __post_root_validators__ = [(False, <function BaseTool.raise_deprecati...\n",
      " |\n",
      " |  __pre_root_validators__ = []\n",
      " |\n",
      " |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      " |\n",
      " |  __schema_cache__ = {}\n",
      " |\n",
      " |  __signature__ = <Signature (name: 'str', func: 'Optional[Callabl...l[C...\n",
      " |\n",
      " |  __validators__ = {}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseTool:\n",
      " |\n",
      " |  __call__(self, tool_input: 'str', callbacks: 'Callbacks' = None) -> 'str'\n",
      " |      Make tool callable.\n",
      " |\n",
      " |  async arun(self, tool_input: 'Union[str, Dict]', verbose: 'Optional[bool]' = None, start_color: 'Optional[str]' = 'green', color: 'Optional[str]' = 'green', callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'Any'\n",
      " |      Run the tool asynchronously.\n",
      " |\n",
      " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      The tool's input schema.\n",
      " |\n",
      " |  invoke(self, input: 'Union[str, Dict]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Any'\n",
      " |      Transform a single input into an output. Override to implement.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: A config to use when invoking the runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details.\n",
      " |\n",
      " |      Returns:\n",
      " |          The output of the runnable.\n",
      " |\n",
      " |  run(self, tool_input: 'Union[str, Dict[str, Any]]', verbose: 'Optional[bool]' = None, start_color: 'Optional[str]' = 'green', color: 'Optional[str]' = 'green', callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'Any'\n",
      " |      Run the tool.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from BaseTool:\n",
      " |\n",
      " |  __init_subclass__(**kwargs: 'Any') -> 'None' from pydantic.v1.main.ModelMetaclass\n",
      " |      Create the definition of the new tool class.\n",
      " |\n",
      " |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseTool:\n",
      " |\n",
      " |  is_single_input\n",
      " |      Whether the tool only accepts a single input.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from BaseTool:\n",
      " |\n",
      " |  Config = <class 'langchain_core.tools.BaseTool.Config'>\n",
      " |      Configuration for this pydantic object.\n",
      " |\n",
      " |\n",
      " |  __orig_bases__ = (langchain_core.runnables.base.RunnableSerializable[t...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |\n",
      " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
      " |\n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |\n",
      " |  to_json(self) -> 'Union[SerializedConstructor, SerializedNotImplemented]'\n",
      " |      Serialize the runnable to JSON.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  __repr_args__(self) -> Any\n",
      " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      " |\n",
      " |      Can either return:\n",
      " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      " |\n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  get_lc_namespace() -> List[str] from pydantic.v1.main.ModelMetaclass\n",
      " |      Get the namespace of the langchain object.\n",
      " |\n",
      " |      For example, if the class is `langchain.llms.openai.OpenAI`, then the\n",
      " |      namespace is [\"langchain\", \"llms\", \"openai\"]\n",
      " |\n",
      " |  is_lc_serializable() -> bool from pydantic.v1.main.ModelMetaclass\n",
      " |      Is this class serializable?\n",
      " |\n",
      " |  lc_id() -> List[str] from pydantic.v1.main.ModelMetaclass\n",
      " |      A unique identifier for this class for serialization purposes.\n",
      " |\n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |\n",
      " |      These attributes must be accepted by the constructor.\n",
      " |\n",
      " |  lc_secrets\n",
      " |      A map of constructor argument names to secret ids.\n",
      " |\n",
      " |      For example,\n",
      " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
      " |\n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |\n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |\n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |\n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |\n",
      " |  dict(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) -> 'DictStrAny'\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> str\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |\n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
      " |\n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |\n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |\n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |\n",
      " |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.utils.Representation:\n",
      " |\n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |\n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_name__(self) -> str\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_str__(self, join_str: str) -> str\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |\n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |\n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |\n",
      " |  async abatch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
      " |\n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |\n",
      " |  assign(self, **kwargs: 'Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the dict output of this runnable.\n",
      " |      Returns a new runnable.\n",
      " |\n",
      " |  async astream(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of astream, which calls ainvoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |\n",
      " |  astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1']\", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      [*Beta*] Generate a stream of events.\n",
      " |\n",
      " |      Use to create an iterator over StreamEvents that provide real-time information\n",
      " |      about the progress of the runnable, including StreamEvents from intermediate\n",
      " |      results.\n",
      " |\n",
      " |      A StreamEvent is a dictionary with the following schema:\n",
      " |\n",
      " |      - ``event``: **str** - Event names are of the\n",
      " |          format: on_[runnable_type]_(start|stream|end).\n",
      " |      - ``name``: **str** - The name of the runnable that generated the event.\n",
      " |      - ``run_id``: **str** - randomly generated ID associated with the given execution of\n",
      " |          the runnable that emitted the event.\n",
      " |          A child runnable that gets invoked as part of the execution of a\n",
      " |          parent runnable is assigned its own unique ID.\n",
      " |      - ``tags``: **Optional[List[str]]** - The tags of the runnable that generated\n",
      " |          the event.\n",
      " |      - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the runnable\n",
      " |          that generated the event.\n",
      " |      - ``data``: **Dict[str, Any]**\n",
      " |\n",
      " |\n",
      " |      Below is a table that illustrates some evens that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
      " |      +======================+==================+=================================+===============================================+=================================================+\n",
      " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | {\"generations\": [...], \"llm_output\": None, ...} |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_stream       | some_tool        | {\"x\": 1, \"y\": \"2\"}              |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_chunk   | [retriever name] | {documents: [...]}              |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | {documents: [...]}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |\n",
      " |      Here are declarations associated with the events shown above:\n",
      " |\n",
      " |      `format_docs`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          def format_docs(docs: List[Document]) -> str:\n",
      " |              '''Format the docs.'''\n",
      " |              return \", \".join([doc.page_content for doc in docs])\n",
      " |\n",
      " |          format_docs = RunnableLambda(format_docs)\n",
      " |\n",
      " |      `some_tool`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          @tool\n",
      " |          def some_tool(x: int, y: str) -> dict:\n",
      " |              '''Some_tool.'''\n",
      " |              return {\"x\": x, \"y\": y}\n",
      " |\n",
      " |      `prompt`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          template = ChatPromptTemplate.from_messages(\n",
      " |              [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |\n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |\n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v1\")\n",
      " |          ]\n",
      " |\n",
      " |          # will produce the following events (run_id has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          version: The version of the schema to use.\n",
      " |                   Currently only version 1 is available.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |          include_names: Only include events from runnables with matching names.\n",
      " |          include_types: Only include events from runnables with matching types.\n",
      " |          include_tags: Only include events from runnables with matching tags.\n",
      " |          exclude_names: Exclude events from runnables with matching names.\n",
      " |          exclude_types: Exclude events from runnables with matching types.\n",
      " |          exclude_tags: Exclude events from runnables with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the runnable.\n",
      " |              These will be passed to astream_log as this implementation\n",
      " |              of astream_events is built on top of astream_log.\n",
      " |\n",
      " |      Returns:\n",
      " |          An async stream of StreamEvents.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |\n",
      " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a runnable, as reported to the callback system.\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |\n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |\n",
      " |      The jsonpatch ops can be applied in order to construct state.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          diff: Whether to yield diffs between each step, or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |\n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of atransform, which buffers input and calls astream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |  batch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |\n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |\n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'Type[BaseModel]'\n",
      " |      The type of config this runnable accepts specified as a pydantic model.\n",
      " |\n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |\n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |\n",
      " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
      " |      Return a graph representation of this runnable.\n",
      " |\n",
      " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
      " |      Get the name of the runnable.\n",
      " |\n",
      " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate output to the runnable.\n",
      " |\n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic output schema that depends on which\n",
      " |      configuration the runnable is invoked with.\n",
      " |\n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate output.\n",
      " |\n",
      " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'List[BasePromptTemplate]'\n",
      " |\n",
      " |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      " |      by calling invoke() with each input.\n",
      " |\n",
      " |  pick(self, keys: 'Union[str, List[str]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the dict output of this runnable.\n",
      " |      Returns a new runnable.\n",
      " |\n",
      " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |\n",
      " |  stream(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of stream, which calls invoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |\n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of transform, which buffers input and then calls stream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a runnable, returning a new Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base runnable\n",
      " |              and its fallbacks must accept a dictionary as input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |\n",
      " |  with_listeners(self, *, on_start: 'Optional[Listener]' = None, on_end: 'Optional[Listener]' = None, on_error: 'Optional[Listener]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      on_start: Called before the runnable starts running, with the Run object.\n",
      " |      on_end: Called after the runnable finishes running, with the Run object.\n",
      " |      on_error: Called if the runnable throws an error, with the Run object.\n",
      " |\n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |\n",
      " |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original runnable on exceptions.\n",
      " |\n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
      " |                                   between retries\n",
      " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original runnable on exceptions.\n",
      " |\n",
      " |  with_types(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  InputType\n",
      " |      The type of input this runnable accepts specified as a type annotation.\n",
      " |\n",
      " |  OutputType\n",
      " |      The type of output this runnable produces specified as a type annotation.\n",
      " |\n",
      " |  config_specs\n",
      " |      List configurable fields for this runnable.\n",
      " |\n",
      " |  input_schema\n",
      " |      The type of input this runnable accepts specified as a pydantic model.\n",
      " |\n",
      " |  output_schema\n",
      " |      The type of output this runnable produces specified as a pydantic model.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  name = None\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |\n",
      " |  __class_getitem__(...) from pydantic.v1.main.ModelMetaclass\n",
      " |      Parameterizes a generic class.\n",
      " |\n",
      " |      At least, parameterizing a generic class is the *main* thing this\n",
      " |      method does. For example, for some generic class `Foo`, this is called\n",
      " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |\n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo[T]: ...`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search = GoogleSearchAPIWrapper()\n",
    "tool = Tool(\n",
    "        name=\"google_search\",\n",
    "        description=\"Search Google for recent results.\",\n",
    "        func=search.run,\n",
    "    )\n",
    "\n",
    "help(tool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d384bb31",
   "metadata": {},
   "source": [
    "### Review with data from Politifact\n",
    "- This data is used to determine how accurate the RAG model is in retrieving the information and informing a judgment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d273871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data has been previously web-scraped and is available in JSON format\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "path = ''\n",
    "fname = 'politifact_3000.json'\n",
    "\n",
    "f = open(path+fname)\n",
    "data = json.load(f)\n",
    "\n",
    "politifact_scraped_data = pd.DataFrame()\n",
    "\n",
    "j=0\n",
    "for i in data:\n",
    "    temp = {}\n",
    "    # if j==0:\n",
    "    for key in i.keys():\n",
    "        temp[key] = str(i[key])\n",
    "\n",
    "    politifact_scraped_data = pd.concat([politifact_scraped_data, pd.DataFrame(data=temp, index=[j])])\n",
    "    j+=1\n",
    " \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34eefbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claim_source</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_author</th>\n",
       "      <th>veracity</th>\n",
       "      <th>review_tags</th>\n",
       "      <th>review_points</th>\n",
       "      <th>review_article</th>\n",
       "      <th>review_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Michael Moore is supporting Trump in the 2024 ...</td>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>March 4, 2024</td>\n",
       "      <td>Sofia Ahmed</td>\n",
       "      <td>false</td>\n",
       "      <td>['Elections', 'Pop Culture', 'Instagram posts']</td>\n",
       "      <td>['In a longer video of the 2016 documentary \"M...</td>\n",
       "      <td>Michael Moore, a liberal filmmaker, did not an...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2024/mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The 2022 CHIPS and Science Act â€œattracted $640...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>February 29, 2024</td>\n",
       "      <td>Louis Jacobson</td>\n",
       "      <td>barely-true</td>\n",
       "      <td>['National', 'Science', 'Technology', 'Joe Bid...</td>\n",
       "      <td>['A Semiconductor Industry Association analysi...</td>\n",
       "      <td>Ahead of his 2024 State of the Union address, ...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2024/feb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>â€œNorth Carolina has the longest voting period ...</td>\n",
       "      <td>Deborah Ross</td>\n",
       "      <td>February 28, 2024</td>\n",
       "      <td>Paul Specht</td>\n",
       "      <td>half-true</td>\n",
       "      <td>['Elections', 'North Carolina', 'Deborah Ross']</td>\n",
       "      <td>[]</td>\n",
       "      <td>A Democratic congresswoman from North Carolina...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2024/feb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>â€œStudy reveals Bill Gatesâ€™ Fake Meat causes â€˜t...</td>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>February 28, 2024</td>\n",
       "      <td>Madison Czopek</td>\n",
       "      <td>false</td>\n",
       "      <td>['Food', 'Health Care', 'Science', 'Health Che...</td>\n",
       "      <td>['PolitiFact found no reliable studies or news...</td>\n",
       "      <td>A truck with a turbocharged engine boasts enha...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2024/feb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Donald Trump called his wife \"Mercedes\" instea...</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>February 28, 2024</td>\n",
       "      <td>Maria Ramirez Uribe</td>\n",
       "      <td>false</td>\n",
       "      <td>['National', 'Facebook Fact-checks', 'Facebook...</td>\n",
       "      <td>['Former President Trump was talking to Merced...</td>\n",
       "      <td>News headlines and social media posts claim fo...</td>\n",
       "      <td>https://www.politifact.com/factchecks/2024/feb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim     claim_source  \\\n",
       "0  Michael Moore is supporting Trump in the 2024 ...  Instagram posts   \n",
       "1  The 2022 CHIPS and Science Act â€œattracted $640...        Joe Biden   \n",
       "2  â€œNorth Carolina has the longest voting period ...     Deborah Ross   \n",
       "3  â€œStudy reveals Bill Gatesâ€™ Fake Meat causes â€˜t...  Instagram posts   \n",
       "4  Donald Trump called his wife \"Mercedes\" instea...   Facebook posts   \n",
       "\n",
       "         review_date        review_author     veracity  \\\n",
       "0      March 4, 2024          Sofia Ahmed        false   \n",
       "1  February 29, 2024       Louis Jacobson  barely-true   \n",
       "2  February 28, 2024          Paul Specht    half-true   \n",
       "3  February 28, 2024       Madison Czopek        false   \n",
       "4  February 28, 2024  Maria Ramirez Uribe        false   \n",
       "\n",
       "                                         review_tags  \\\n",
       "0    ['Elections', 'Pop Culture', 'Instagram posts']   \n",
       "1  ['National', 'Science', 'Technology', 'Joe Bid...   \n",
       "2    ['Elections', 'North Carolina', 'Deborah Ross']   \n",
       "3  ['Food', 'Health Care', 'Science', 'Health Che...   \n",
       "4  ['National', 'Facebook Fact-checks', 'Facebook...   \n",
       "\n",
       "                                       review_points  \\\n",
       "0  ['In a longer video of the 2016 documentary \"M...   \n",
       "1  ['A Semiconductor Industry Association analysi...   \n",
       "2                                                 []   \n",
       "3  ['PolitiFact found no reliable studies or news...   \n",
       "4  ['Former President Trump was talking to Merced...   \n",
       "\n",
       "                                      review_article  \\\n",
       "0  Michael Moore, a liberal filmmaker, did not an...   \n",
       "1  Ahead of his 2024 State of the Union address, ...   \n",
       "2  A Democratic congresswoman from North Carolina...   \n",
       "3  A truck with a turbocharged engine boasts enha...   \n",
       "4  News headlines and social media posts claim fo...   \n",
       "\n",
       "                                          review_url  \n",
       "0  https://www.politifact.com/factchecks/2024/mar...  \n",
       "1  https://www.politifact.com/factchecks/2024/feb...  \n",
       "2  https://www.politifact.com/factchecks/2024/feb...  \n",
       "3  https://www.politifact.com/factchecks/2024/feb...  \n",
       "4  https://www.politifact.com/factchecks/2024/feb...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politifact_scraped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c1fda70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "veracity\n",
       "false          1781\n",
       "pants-fire      539\n",
       "barely-true     261\n",
       "half-true       173\n",
       "mostly-true     157\n",
       "true             71\n",
       "full-flop        12\n",
       "half-flip         6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politifact_scraped_data['veracity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd955f3",
   "metadata": {},
   "source": [
    "##### We will remove categories full-flop, half-flip, and no-flip because these are not ones we are interested in being able to judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e281ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "politifact_scraped_data = politifact_scraped_data[~politifact_scraped_data['veracity'].isin(['no-flip','half-flip','full-flop'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6024bb",
   "metadata": {},
   "source": [
    "##### We also want to distill categories into relevant information\n",
    "\n",
    "- From polificat:\n",
    "   - TRUE â€“ The statement is accurate and thereâ€™s nothing significant missing.\n",
    "   - MOSTLY TRUE â€“ The statement is accurate but needs clarification or additional information.\n",
    "   - HALF TRUE â€“ The statement is partially accurate but leaves out important details or takes things out of context.\n",
    "   - MOSTLY FALSE â€“ The statement contains an element of truth but ignores critical facts that would give a different impression.\n",
    "   - FALSE â€“ The statement is not accurate.\n",
    "   - PANTS ON FIRE â€“ The statement is not accurate and makes a ridiculous claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f656d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "veracity_simplified\n",
       "false    2581\n",
       "true      401\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplifed_categories = {\n",
    "    'false': 'false',\n",
    "    'pantsfire': 'false',\n",
    "    'barelytrue': 'false',\n",
    "    'halftrue': 'true',\n",
    "    'mostlytrue': 'true',\n",
    "    'true': 'true'\n",
    "}\n",
    "\n",
    "politifact_scraped_data['veracity_simplified'] = politifact_scraped_data['veracity'].apply(lambda x: simplifed_categories[x.lower().replace('-','').replace(' ','')])\n",
    "politifact_scraped_data['veracity_simplified'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "318f7df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Michael Moore is supporting Trump in the 2024 election.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politifact_scraped_data['claim'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6eaae7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Elections', 'Pop Culture', 'Instagram posts']\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politifact_scraped_data['review_tags'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a5c5b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\'In a longer video of the 2016 documentary \"Michael Moore In TrumpLand,\" Moore tells members of an Ohio audience that they will regret voting Donald Trump for president.\\\\xa0\\', \\'Moore said he voted for Joe Biden in 2020.\\\\xa0\\', \\'Learn more about PolitiFactâ€™s \\']'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politifact_scraped_data['review_points'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "205a8df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8150129318237305\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "test = google_search_agent('Michael Moore is supporting Trump in the 2024 election.', response_option='minimal')\n",
    "print(test[1])\n",
    "print(test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3703856c",
   "metadata": {},
   "source": [
    "##### Use a subset of the data to ensure we don't hit the API limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40026381",
   "metadata": {},
   "outputs": [],
   "source": [
    "politifact_scraped_data_subset = politifact_scraped_data.iloc[50:145]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a5fbdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(politifact_scraped_data_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771c114",
   "metadata": {},
   "source": [
    "##### We want to assess the workflow\n",
    "- 1) ensure naming is correct and is calling the right api\n",
    "    - API v1: encyclopedias and politifact\n",
    "    - API v2: encyclopedias only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7599c7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2003/2675367198.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  politifact_scraped_data_subset['judgement_apiv0'] = politifact_scraped_data_subset.apply(lambda x: str(google_search_agent(x['claim'], response_option='minimal')), axis=1)\n"
     ]
    }
   ],
   "source": [
    "politifact_scraped_data_subset['judgement_apiv0'] = politifact_scraped_data_subset.apply(lambda x: str(google_search_agent(x['claim'], response_option='minimal')), axis=1)\n",
    "# politifact_scraped_data_subset['judgement_apiv1'] = politifact_scraped_data_subset.apply(lambda x: str(google_search_agent(x['claim'], response_option='minimal')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98e911ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "politifact_scraped_data_subset.to_csv('politifact_scraped_data_subset_v2_iloc50-145.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90bfc5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veracity</th>\n",
       "      <th>judgement_apiv0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>barely-true</td>\n",
       "      <td>('Answer: False\\n\\nJustification: The claim th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>false</td>\n",
       "      <td>('False', 1.7991347312927246, '-', '-')</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mostly-true</td>\n",
       "      <td>('Answer: True\\n\\nJustification: According to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>false</td>\n",
       "      <td>('Answer: True\\n\\nSource:\\nThe Daily Mail, \"Ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>false</td>\n",
       "      <td>('Answer: False\\n\\nJustification: Based on the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       veracity                                    judgement_apiv0\n",
       "50  barely-true  ('Answer: False\\n\\nJustification: The claim th...\n",
       "51        false            ('False', 1.7991347312927246, '-', '-')\n",
       "52  mostly-true  ('Answer: True\\n\\nJustification: According to ...\n",
       "53        false  ('Answer: True\\n\\nSource:\\nThe Daily Mail, \"Ki...\n",
       "54        false  ('Answer: False\\n\\nJustification: Based on the..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politifact_scraped_data_subset[['veracity','judgement_apiv0']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fea97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "politifact_scraped_data_subset['judgement_apiv1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "159eaaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "politifact_scraped_data_subset.to_csv('politifact_judged_first_50_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def6d0f",
   "metadata": {},
   "source": [
    "##### Assess baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ce76efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_427/3312963526.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  politifact_scraped_data_subset['judgement_baseline'] = politifact_scraped_data_subset.apply(lambda x: str(google_search_agent(x['claim'], response_option='minimal', model_option='baseline')), axis=1)\n"
     ]
    }
   ],
   "source": [
    "politifact_scraped_data_subset['judgement_baseline'] = politifact_scraped_data_subset.apply(lambda x: str(google_search_agent(x['claim'], response_option='minimal', model_option='baseline')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "99e5a9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veracity_simplified</th>\n",
       "      <th>judgement_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>false</td>\n",
       "      <td>(\"False. Michael Moore has not expressed suppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>false</td>\n",
       "      <td>('True', 0.2974367141723633)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>true</td>\n",
       "      <td>(\"Answer: False.\\n\\nNorth Carolina does not ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>false</td>\n",
       "      <td>('Answer: False.\\n\\nThere is no credible scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>false</td>\n",
       "      <td>('True', 0.2951786518096924)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  veracity_simplified                                 judgement_baseline\n",
       "0               false  (\"False. Michael Moore has not expressed suppo...\n",
       "1               false                       ('True', 0.2974367141723633)\n",
       "2                true  (\"Answer: False.\\n\\nNorth Carolina does not ha...\n",
       "3               false  ('Answer: False.\\n\\nThere is no credible scien...\n",
       "4               false                       ('True', 0.2951786518096924)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politifact_scraped_data_subset[['veracity_simplified','judgement_baseline']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "855d2f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>claim</th>\n",
       "      <th>claim_source</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_author</th>\n",
       "      <th>veracity</th>\n",
       "      <th>review_tags</th>\n",
       "      <th>review_points</th>\n",
       "      <th>review_article</th>\n",
       "      <th>...</th>\n",
       "      <th>answer_apiv1</th>\n",
       "      <th>answer_baseline</th>\n",
       "      <th>answer_apiv2</th>\n",
       "      <th>answer_apiv3</th>\n",
       "      <th>answer_apiv0</th>\n",
       "      <th>judgement_apiv1</th>\n",
       "      <th>judgement_baseline</th>\n",
       "      <th>judgement_apiv2</th>\n",
       "      <th>judgement_apiv3</th>\n",
       "      <th>judgement_apiv0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Michael Moore is supporting Trump in the 2024 ...</td>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>4-Mar-24</td>\n",
       "      <td>Sofia Ahmed</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>['Elections', 'Pop Culture', 'Instagram posts']</td>\n",
       "      <td>['In a longer video of the 2016 documentary \"M...</td>\n",
       "      <td>Michael Moore, a liberal filmmaker, did not an...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>('False', 0.7105515003204346)</td>\n",
       "      <td>(\"False. Michael Moore has not expressed suppo...</td>\n",
       "      <td>('False.\\n\\nMichael Moore has never supported ...</td>\n",
       "      <td>('False.\\n\\nJustification: Michael Moore has n...</td>\n",
       "      <td>('False', 2.1150131225585938)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>The 2022 CHIPS and Science Act â€œattracted $640...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>29-Feb-24</td>\n",
       "      <td>Louis Jacobson</td>\n",
       "      <td>barely-true</td>\n",
       "      <td>['National', 'Science', 'Technology', 'Joe Bid...</td>\n",
       "      <td>['A Semiconductor Industry Association analysi...</td>\n",
       "      <td>Ahead of his 2024 State of the Union address, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>('False', 2.0083658695220947)</td>\n",
       "      <td>('True', 0.2974367141723633)</td>\n",
       "      <td>('True', 0.6764819622039795)</td>\n",
       "      <td>('True', 1.0125279426574707)</td>\n",
       "      <td>('True', 1.1109130382537842)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>â€œNorth Carolina has the longest voting period ...</td>\n",
       "      <td>Deborah Ross</td>\n",
       "      <td>28-Feb-24</td>\n",
       "      <td>Paul Specht</td>\n",
       "      <td>half-true</td>\n",
       "      <td>['Elections', 'North Carolina', 'Deborah Ross']</td>\n",
       "      <td>[]</td>\n",
       "      <td>A Democratic congresswoman from North Carolina...</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>('Answer: True\\n\\nJustification: North Carolin...</td>\n",
       "      <td>(\"Answer: False.\\n\\nNorth Carolina does not ha...</td>\n",
       "      <td>('Answer: False\\n\\nJustification: North Caroli...</td>\n",
       "      <td>('True', 0.7923736572265625)</td>\n",
       "      <td>('False', 1.1261639595031738)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>â€œStudy reveals Bill Gatesâ€™ Fake Meat causes â€˜t...</td>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>28-Feb-24</td>\n",
       "      <td>Madison Czopek</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>['Food', 'Health Care', 'Science', 'Health Che...</td>\n",
       "      <td>['PolitiFact found no reliable studies or news...</td>\n",
       "      <td>A truck with a turbocharged engine boasts enha...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>('Answer: False\\n\\nJustification: There is no ...</td>\n",
       "      <td>('Answer: False.\\n\\nThere is no credible scien...</td>\n",
       "      <td>('False.\\n\\nThere is no scientific evidence to...</td>\n",
       "      <td>('Answer: False.\\n\\nThere is no credible evide...</td>\n",
       "      <td>('Answer: False\\n\\nJustification: There is no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Donald Trump called his wife \"Mercedes\" instea...</td>\n",
       "      <td>Facebook posts</td>\n",
       "      <td>28-Feb-24</td>\n",
       "      <td>Maria Ramirez Uribe</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>['National', 'Facebook Fact-checks', 'Facebook...</td>\n",
       "      <td>['Former President Trump was talking to Merced...</td>\n",
       "      <td>News headlines and social media posts claim fo...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>('False', 1.9021382331848145)</td>\n",
       "      <td>('True', 0.2951786518096924)</td>\n",
       "      <td>('True', 0.7099990844726562)</td>\n",
       "      <td>('False.\\n\\nDonald Trump did not call his wife...</td>\n",
       "      <td>('False', 1.7606425285339355)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  \\\n",
       "0             0           0   \n",
       "1             1           1   \n",
       "2             2           2   \n",
       "3             3           3   \n",
       "4             4           4   \n",
       "\n",
       "                                               claim     claim_source  \\\n",
       "0  Michael Moore is supporting Trump in the 2024 ...  Instagram posts   \n",
       "1  The 2022 CHIPS and Science Act â€œattracted $640...        Joe Biden   \n",
       "2  â€œNorth Carolina has the longest voting period ...     Deborah Ross   \n",
       "3  â€œStudy reveals Bill Gatesâ€™ Fake Meat causes â€˜t...  Instagram posts   \n",
       "4  Donald Trump called his wife \"Mercedes\" instea...   Facebook posts   \n",
       "\n",
       "  review_date        review_author     veracity  \\\n",
       "0    4-Mar-24          Sofia Ahmed        FALSE   \n",
       "1   29-Feb-24       Louis Jacobson  barely-true   \n",
       "2   28-Feb-24          Paul Specht    half-true   \n",
       "3   28-Feb-24       Madison Czopek        FALSE   \n",
       "4   28-Feb-24  Maria Ramirez Uribe        FALSE   \n",
       "\n",
       "                                         review_tags  \\\n",
       "0    ['Elections', 'Pop Culture', 'Instagram posts']   \n",
       "1  ['National', 'Science', 'Technology', 'Joe Bid...   \n",
       "2    ['Elections', 'North Carolina', 'Deborah Ross']   \n",
       "3  ['Food', 'Health Care', 'Science', 'Health Che...   \n",
       "4  ['National', 'Facebook Fact-checks', 'Facebook...   \n",
       "\n",
       "                                       review_points  \\\n",
       "0  ['In a longer video of the 2016 documentary \"M...   \n",
       "1  ['A Semiconductor Industry Association analysi...   \n",
       "2                                                 []   \n",
       "3  ['PolitiFact found no reliable studies or news...   \n",
       "4  ['Former President Trump was talking to Merced...   \n",
       "\n",
       "                                      review_article  ... answer_apiv1  \\\n",
       "0  Michael Moore, a liberal filmmaker, did not an...  ...        False   \n",
       "1  Ahead of his 2024 State of the Union address, ...  ...        False   \n",
       "2  A Democratic congresswoman from North Carolina...  ...         True   \n",
       "3  A truck with a turbocharged engine boasts enha...  ...        False   \n",
       "4  News headlines and social media posts claim fo...  ...        False   \n",
       "\n",
       "   answer_baseline  answer_apiv2 answer_apiv3 answer_apiv0  \\\n",
       "0            False         False        False        False   \n",
       "1             True          True         True         True   \n",
       "2            False         False         True        False   \n",
       "3            False         False        False        False   \n",
       "4             True          True        False        False   \n",
       "\n",
       "                                     judgement_apiv1  \\\n",
       "0                      ('False', 0.7105515003204346)   \n",
       "1                      ('False', 2.0083658695220947)   \n",
       "2  ('Answer: True\\n\\nJustification: North Carolin...   \n",
       "3  ('Answer: False\\n\\nJustification: There is no ...   \n",
       "4                      ('False', 1.9021382331848145)   \n",
       "\n",
       "                                  judgement_baseline  \\\n",
       "0  (\"False. Michael Moore has not expressed suppo...   \n",
       "1                       ('True', 0.2974367141723633)   \n",
       "2  (\"Answer: False.\\n\\nNorth Carolina does not ha...   \n",
       "3  ('Answer: False.\\n\\nThere is no credible scien...   \n",
       "4                       ('True', 0.2951786518096924)   \n",
       "\n",
       "                                     judgement_apiv2  \\\n",
       "0  ('False.\\n\\nMichael Moore has never supported ...   \n",
       "1                       ('True', 0.6764819622039795)   \n",
       "2  ('Answer: False\\n\\nJustification: North Caroli...   \n",
       "3  ('False.\\n\\nThere is no scientific evidence to...   \n",
       "4                       ('True', 0.7099990844726562)   \n",
       "\n",
       "                                     judgement_apiv3  \\\n",
       "0  ('False.\\n\\nJustification: Michael Moore has n...   \n",
       "1                       ('True', 1.0125279426574707)   \n",
       "2                       ('True', 0.7923736572265625)   \n",
       "3  ('Answer: False.\\n\\nThere is no credible evide...   \n",
       "4  ('False.\\n\\nDonald Trump did not call his wife...   \n",
       "\n",
       "                                     judgement_apiv0  \n",
       "0                      ('False', 2.1150131225585938)  \n",
       "1                       ('True', 1.1109130382537842)  \n",
       "2                      ('False', 1.1261639595031738)  \n",
       "3  ('Answer: False\\n\\nJustification: There is no ...  \n",
       "4                      ('False', 1.7606425285339355)  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "politifact_scraped_data_subset = pd.read_csv('politifact_judged_first_50_new.csv')\n",
    "politifact_scraped_data_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddbcd72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# politifact_scraped_data_subset['judgement_apiv2'] = politifact_scraped_data_subset.apply(lambda x: str(google_search_agent(x['claim'], response_option='minimal')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "703d679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# politifact_scraped_data_subset['judgement_apiv3'] = politifact_scraped_data_subset.apply(lambda x: str(google_search_agent(x['claim'], response_option='minimal')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63cb24ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# politifact_scraped_data_subset['judgement_apiv0'] = politifact_scraped_data_subset.apply(lambda x: str(google_search_agent(x['claim'], response_option='minimal')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f05e068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veracity_simplified</th>\n",
       "      <th>judgement_apiv2</th>\n",
       "      <th>judgement_apiv3</th>\n",
       "      <th>judgement_apiv0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>('False.\\n\\nMichael Moore has never supported ...</td>\n",
       "      <td>('False.\\n\\nJustification: Michael Moore has n...</td>\n",
       "      <td>('False', 2.1150131225585938)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>('True', 0.6764819622039795)</td>\n",
       "      <td>('True', 1.0125279426574707)</td>\n",
       "      <td>('True', 1.1109130382537842)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>('Answer: False\\n\\nJustification: North Caroli...</td>\n",
       "      <td>('True', 0.7923736572265625)</td>\n",
       "      <td>('False', 1.1261639595031738)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>('False.\\n\\nThere is no scientific evidence to...</td>\n",
       "      <td>('Answer: False.\\n\\nThere is no credible evide...</td>\n",
       "      <td>('Answer: False\\n\\nJustification: There is no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>('True', 0.7099990844726562)</td>\n",
       "      <td>('False.\\n\\nDonald Trump did not call his wife...</td>\n",
       "      <td>('False', 1.7606425285339355)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   veracity_simplified                                    judgement_apiv2  \\\n",
       "0                False  ('False.\\n\\nMichael Moore has never supported ...   \n",
       "1                False                       ('True', 0.6764819622039795)   \n",
       "2                 True  ('Answer: False\\n\\nJustification: North Caroli...   \n",
       "3                False  ('False.\\n\\nThere is no scientific evidence to...   \n",
       "4                False                       ('True', 0.7099990844726562)   \n",
       "\n",
       "                                     judgement_apiv3  \\\n",
       "0  ('False.\\n\\nJustification: Michael Moore has n...   \n",
       "1                       ('True', 1.0125279426574707)   \n",
       "2                       ('True', 0.7923736572265625)   \n",
       "3  ('Answer: False.\\n\\nThere is no credible evide...   \n",
       "4  ('False.\\n\\nDonald Trump did not call his wife...   \n",
       "\n",
       "                                     judgement_apiv0  \n",
       "0                      ('False', 2.1150131225585938)  \n",
       "1                       ('True', 1.1109130382537842)  \n",
       "2                      ('False', 1.1261639595031738)  \n",
       "3  ('Answer: False\\n\\nJustification: There is no ...  \n",
       "4                      ('False', 1.7606425285339355)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politifact_scraped_data_subset[['veracity_simplified','judgement_apiv2','judgement_apiv3','judgement_apiv0']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ae07db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# float(politifact_scraped_data_subset['judgement_apiv1'].iloc[3].replace('(','').replace(')','').split(',')[-1].replace(' ',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c342916",
   "metadata": {},
   "outputs": [],
   "source": [
    "politifact_scraped_data_subset['time_baseline'] = politifact_scraped_data_subset['judgement_baseline'].apply(lambda x: float(x.replace('(','').replace(')','').split(',')[-1].replace(' ','')))\n",
    "politifact_scraped_data_subset['time_apiv0'] = politifact_scraped_data_subset['judgement_apiv0'].apply(lambda x: float(x.replace('(','').replace(')','').split(',')[-1].replace(' ','')))\n",
    "politifact_scraped_data_subset['time_apiv1'] = politifact_scraped_data_subset['judgement_apiv1'].apply(lambda x: float(x.replace('(','').replace(')','').split(',')[-1].replace(' ','')))\n",
    "politifact_scraped_data_subset['time_apiv2'] = politifact_scraped_data_subset['judgement_apiv2'].apply(lambda x: float(x.replace('(','').replace(')','').split(',')[-1].replace(' ','')))\n",
    "politifact_scraped_data_subset['time_apiv3'] = politifact_scraped_data_subset['judgement_apiv3'].apply(lambda x: float(x.replace('(','').replace(')','').split(',')[-1].replace(' ','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df8aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7376cc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "politifact_scraped_data_subset['correct_baseline'] = politifact_scraped_data_subset.apply(lambda x: True if x['answer_baseline'] == x['veracity_simplified'] else False if x['answer_baseline'] is not None else None, axis=1)\n",
    "politifact_scraped_data_subset['correct_apiv0'] = politifact_scraped_data_subset.apply(lambda x: True if x['answer_apiv0'] == x['veracity_simplified'] else False if x['answer_apiv0'] is not None else None, axis=1)\n",
    "politifact_scraped_data_subset['correct_apiv1'] = politifact_scraped_data_subset.apply(lambda x: True if x['answer_apiv1'] == x['veracity_simplified'] else False if x['answer_apiv1'] is not None else None, axis=1)\n",
    "politifact_scraped_data_subset['correct_apiv2'] = politifact_scraped_data_subset.apply(lambda x: True if x['answer_apiv2'] == x['veracity_simplified'] else False if x['answer_apiv2'] is not None else None, axis=1)\n",
    "politifact_scraped_data_subset['correct_apiv3'] = politifact_scraped_data_subset.apply(lambda x: True if x['answer_apiv3'] == x['veracity_simplified'] else False if x['answer_apiv3'] is not None else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e01108c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trial</th>\n",
       "      <th>Average Time</th>\n",
       "      <th>Std Time</th>\n",
       "      <th>Number Correct</th>\n",
       "      <th>Total</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>5.897520</td>\n",
       "      <td>2.911105</td>\n",
       "      <td>33</td>\n",
       "      <td>50</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apiv0</td>\n",
       "      <td>5.839792</td>\n",
       "      <td>3.939716</td>\n",
       "      <td>43</td>\n",
       "      <td>50</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apiv1</td>\n",
       "      <td>6.026972</td>\n",
       "      <td>3.882333</td>\n",
       "      <td>46</td>\n",
       "      <td>50</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apiv2</td>\n",
       "      <td>6.459515</td>\n",
       "      <td>4.073244</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apiv3</td>\n",
       "      <td>6.941493</td>\n",
       "      <td>3.910810</td>\n",
       "      <td>37</td>\n",
       "      <td>50</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Trial  Average Time  Std Time  Number Correct  Total  score\n",
       "0  baseline      5.897520  2.911105              33     50   0.66\n",
       "0     apiv0      5.839792  3.939716              43     50   0.86\n",
       "0     apiv1      6.026972  3.882333              46     50   0.92\n",
       "0     apiv2      6.459515  4.073244              32     50   0.64\n",
       "0     apiv3      6.941493  3.910810              37     50   0.74"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.DataFrame()\n",
    "\n",
    "for val in ['baseline','apiv0','apiv1','apiv2','apiv3']:\n",
    "\n",
    "    \n",
    "    summary = pd.concat(\n",
    "        [\n",
    "            summary, \n",
    "            pd.DataFrame(\n",
    "                data={\n",
    "                    'Trial':val, \n",
    "                    'Average Time': politifact_scraped_data_subset['time_'+val].mean(), \n",
    "                    'Std Time': politifact_scraped_data_subset['time_'+val].std(),\n",
    "                    'Number Correct': politifact_scraped_data_subset['correct_'+val].sum(),\n",
    "                    'Total': len(politifact_scraped_data_subset[politifact_scraped_data_subset['answer_'+val] != \"NA\"])\n",
    "                    }, index=[0]\n",
    "                )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "summary['score'] = summary['Number Correct'] / summary['Total']\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "889277a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer_baseline\n",
       "False    34\n",
       "True     12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politifact_scraped_data_subset['answer_baseline'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00dc58ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "3.992983102798462\n"
     ]
    }
   ],
   "source": [
    "text = \"Trump is unable to get a bond for $1 million\"\n",
    "\n",
    "out = google_search_agent(text, response_option='minimal', return_information=True)\n",
    "print(out[0])\n",
    "print(out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9f1f20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<3 days ago ... Getty Images Donald Trump Getty Images. Mr Trump's lawyers say they have approached dozens of bond companies but cannot secure one. DonaldÂ ... 1 day ago ... Former President Donald Trump has not been able to get a bond to secure the $464 million civil fraud judgment against him and hisÂ ... 1 day ago ... Former U.S. President Donald Trump holds up a news story about New York Attorney General Letitia James as he speaks to the media at one of hisÂ ... 24 hours ago ... Former President Donald Trump can't find an insurance company to underwrite his bond to cover the massive judgment against him in the NewÂ ... 17 hours ago ... New York's attorney general, Letitia James, who brought the fraud case, would be entitled to collect the $454 million and could seek to seize MrÂ ... 19 hours ago ... Donald Trump's efforts to secure a bond to cover a $454 million ... one of several legal travails the ... get a bond, wrote in the court filing thatÂ ... 1 day ago ... The former president's lawyers said in a court filing that Trump and the Trump Organization have been unable to get a surety company toÂ ... 1 day ago ... ... bond. A private company like the Trump Organization would need $1 billion in cash to obtain the bond and to continue to operate, an amountÂ ... Updated 1:34 PM PDT, March 18 ... To obtain a bond, they would be required to post collateral covering 120% of the judgment, or about $557.5 million, Trump's lawyers said. ... A full bond is necessary, Fan wrote, in part because Trump'sÂ ... Feb 22, 2024 ... ... get eaten up by his court ... â€œIf he can't post a bond or meet the appellate division's bonding ... Weisselberg was ordered to pay $1 millionÂ ...>\n"
     ]
    }
   ],
   "source": [
    "print(out[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb2cee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='google_search' description='Search Google for recent results.' func=<bound method GoogleSearchAPIWrapper.run of GoogleSearchAPIWrapper(search_engine=<googleapiclient.discovery.Resource object at 0x7f4c285341d0>, google_api_key='AIzaSyDPtBvSXiWGW5I6vPiaAL9LTip2AUX_MsY', google_cse_id='318aad6b62ac04395', k=10, siterestrict=False)>\n"
     ]
    }
   ],
   "source": [
    "print(out[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13871d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Tool in module langchain_core.tools object:\n",
      "\n",
      "class Tool(BaseTool)\n",
      " |  Tool(name: 'str', func: 'Optional[Callable]', description: 'str', *, args_schema: Optional[Type[pydantic.v1.main.BaseModel]] = None, return_direct: bool = False, verbose: bool = False, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, handle_tool_error: Union[bool, str, Callable[[langchain_core.tools.ToolException], str], NoneType] = False, handle_validation_error: Union[bool, str, Callable[[pydantic.v1.error_wrappers.ValidationError], str], NoneType] = False, coroutine: Optional[Callable[..., Awaitable[str]]] = None) -> None\n",
      " |\n",
      " |  Tool that takes in function or coroutine directly.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Tool\n",
      " |      BaseTool\n",
      " |      langchain_core.runnables.base.RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.v1.main.BaseModel\n",
      " |      pydantic.v1.utils.Representation\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, name: 'str', func: 'Optional[Callable]', description: 'str', **kwargs: 'Any') -> 'None'\n",
      " |      Initialize tool.\n",
      " |\n",
      " |  async ainvoke(self, input: 'Union[str, Dict]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Any'\n",
      " |      Default implementation of ainvoke, calls invoke from a thread.\n",
      " |\n",
      " |      The default implementation allows usage of async code even if\n",
      " |      the runnable did not implement a native async version of invoke.\n",
      " |\n",
      " |      Subclasses should override this method if they can run asynchronously.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  from_function(func: 'Optional[Callable]', name: 'str', description: 'str', return_direct: 'bool' = False, args_schema: 'Optional[Type[BaseModel]]' = None, coroutine: 'Optional[Callable[..., Awaitable[Any]]]' = None, **kwargs: 'Any') -> 'Tool' from pydantic.v1.main.ModelMetaclass\n",
      " |      Initialize tool from a function.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  args\n",
      " |      The tool's input arguments.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'coroutine': 'Optional[Callable[..., Awaitable[str]...\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
      " |\n",
      " |  __custom_root_type__ = False\n",
      " |\n",
      " |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True}\n",
      " |\n",
      " |  __fields__ = {'args_schema': ModelField(name='args_schema', type=Optio...\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __include_fields__ = None\n",
      " |\n",
      " |  __parameters__ = ()\n",
      " |\n",
      " |  __post_root_validators__ = [(False, <function BaseTool.raise_deprecati...\n",
      " |\n",
      " |  __pre_root_validators__ = []\n",
      " |\n",
      " |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      " |\n",
      " |  __schema_cache__ = {}\n",
      " |\n",
      " |  __signature__ = <Signature (name: 'str', func: 'Optional[Callabl...l[C...\n",
      " |\n",
      " |  __validators__ = {}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseTool:\n",
      " |\n",
      " |  __call__(self, tool_input: 'str', callbacks: 'Callbacks' = None) -> 'str'\n",
      " |      Make tool callable.\n",
      " |\n",
      " |  async arun(self, tool_input: 'Union[str, Dict]', verbose: 'Optional[bool]' = None, start_color: 'Optional[str]' = 'green', color: 'Optional[str]' = 'green', callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'Any'\n",
      " |      Run the tool asynchronously.\n",
      " |\n",
      " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      The tool's input schema.\n",
      " |\n",
      " |  invoke(self, input: 'Union[str, Dict]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Any'\n",
      " |      Transform a single input into an output. Override to implement.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: A config to use when invoking the runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details.\n",
      " |\n",
      " |      Returns:\n",
      " |          The output of the runnable.\n",
      " |\n",
      " |  run(self, tool_input: 'Union[str, Dict[str, Any]]', verbose: 'Optional[bool]' = None, start_color: 'Optional[str]' = 'green', color: 'Optional[str]' = 'green', callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'Any'\n",
      " |      Run the tool.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from BaseTool:\n",
      " |\n",
      " |  __init_subclass__(**kwargs: 'Any') -> 'None' from pydantic.v1.main.ModelMetaclass\n",
      " |      Create the definition of the new tool class.\n",
      " |\n",
      " |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseTool:\n",
      " |\n",
      " |  is_single_input\n",
      " |      Whether the tool only accepts a single input.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from BaseTool:\n",
      " |\n",
      " |  Config = <class 'langchain_core.tools.BaseTool.Config'>\n",
      " |      Configuration for this pydantic object.\n",
      " |\n",
      " |\n",
      " |  __orig_bases__ = (langchain_core.runnables.base.RunnableSerializable[t...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |\n",
      " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
      " |\n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |\n",
      " |  to_json(self) -> 'Union[SerializedConstructor, SerializedNotImplemented]'\n",
      " |      Serialize the runnable to JSON.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  __repr_args__(self) -> Any\n",
      " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      " |\n",
      " |      Can either return:\n",
      " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      " |\n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  get_lc_namespace() -> List[str] from pydantic.v1.main.ModelMetaclass\n",
      " |      Get the namespace of the langchain object.\n",
      " |\n",
      " |      For example, if the class is `langchain.llms.openai.OpenAI`, then the\n",
      " |      namespace is [\"langchain\", \"llms\", \"openai\"]\n",
      " |\n",
      " |  is_lc_serializable() -> bool from pydantic.v1.main.ModelMetaclass\n",
      " |      Is this class serializable?\n",
      " |\n",
      " |  lc_id() -> List[str] from pydantic.v1.main.ModelMetaclass\n",
      " |      A unique identifier for this class for serialization purposes.\n",
      " |\n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |\n",
      " |      These attributes must be accepted by the constructor.\n",
      " |\n",
      " |  lc_secrets\n",
      " |      A map of constructor argument names to secret ids.\n",
      " |\n",
      " |      For example,\n",
      " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
      " |\n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |\n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |\n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |\n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |\n",
      " |  dict(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) -> 'DictStrAny'\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> str\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |\n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
      " |\n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |\n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |\n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |\n",
      " |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.utils.Representation:\n",
      " |\n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |\n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_name__(self) -> str\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_str__(self, join_str: str) -> str\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |\n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |\n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |\n",
      " |  async abatch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
      " |\n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |\n",
      " |  assign(self, **kwargs: 'Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the dict output of this runnable.\n",
      " |      Returns a new runnable.\n",
      " |\n",
      " |  async astream(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of astream, which calls ainvoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |\n",
      " |  astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1']\", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      [*Beta*] Generate a stream of events.\n",
      " |\n",
      " |      Use to create an iterator over StreamEvents that provide real-time information\n",
      " |      about the progress of the runnable, including StreamEvents from intermediate\n",
      " |      results.\n",
      " |\n",
      " |      A StreamEvent is a dictionary with the following schema:\n",
      " |\n",
      " |      - ``event``: **str** - Event names are of the\n",
      " |          format: on_[runnable_type]_(start|stream|end).\n",
      " |      - ``name``: **str** - The name of the runnable that generated the event.\n",
      " |      - ``run_id``: **str** - randomly generated ID associated with the given execution of\n",
      " |          the runnable that emitted the event.\n",
      " |          A child runnable that gets invoked as part of the execution of a\n",
      " |          parent runnable is assigned its own unique ID.\n",
      " |      - ``tags``: **Optional[List[str]]** - The tags of the runnable that generated\n",
      " |          the event.\n",
      " |      - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the runnable\n",
      " |          that generated the event.\n",
      " |      - ``data``: **Dict[str, Any]**\n",
      " |\n",
      " |\n",
      " |      Below is a table that illustrates some evens that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
      " |      +======================+==================+=================================+===============================================+=================================================+\n",
      " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | {\"generations\": [...], \"llm_output\": None, ...} |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_stream       | some_tool        | {\"x\": 1, \"y\": \"2\"}              |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_chunk   | [retriever name] | {documents: [...]}              |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | {documents: [...]}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |\n",
      " |      Here are declarations associated with the events shown above:\n",
      " |\n",
      " |      `format_docs`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          def format_docs(docs: List[Document]) -> str:\n",
      " |              '''Format the docs.'''\n",
      " |              return \", \".join([doc.page_content for doc in docs])\n",
      " |\n",
      " |          format_docs = RunnableLambda(format_docs)\n",
      " |\n",
      " |      `some_tool`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          @tool\n",
      " |          def some_tool(x: int, y: str) -> dict:\n",
      " |              '''Some_tool.'''\n",
      " |              return {\"x\": x, \"y\": y}\n",
      " |\n",
      " |      `prompt`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          template = ChatPromptTemplate.from_messages(\n",
      " |              [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |\n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |\n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v1\")\n",
      " |          ]\n",
      " |\n",
      " |          # will produce the following events (run_id has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          version: The version of the schema to use.\n",
      " |                   Currently only version 1 is available.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |          include_names: Only include events from runnables with matching names.\n",
      " |          include_types: Only include events from runnables with matching types.\n",
      " |          include_tags: Only include events from runnables with matching tags.\n",
      " |          exclude_names: Exclude events from runnables with matching names.\n",
      " |          exclude_types: Exclude events from runnables with matching types.\n",
      " |          exclude_tags: Exclude events from runnables with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the runnable.\n",
      " |              These will be passed to astream_log as this implementation\n",
      " |              of astream_events is built on top of astream_log.\n",
      " |\n",
      " |      Returns:\n",
      " |          An async stream of StreamEvents.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |\n",
      " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a runnable, as reported to the callback system.\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |\n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |\n",
      " |      The jsonpatch ops can be applied in order to construct state.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          diff: Whether to yield diffs between each step, or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |\n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of atransform, which buffers input and calls astream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |  batch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |\n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |\n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'Type[BaseModel]'\n",
      " |      The type of config this runnable accepts specified as a pydantic model.\n",
      " |\n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |\n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |\n",
      " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
      " |      Return a graph representation of this runnable.\n",
      " |\n",
      " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
      " |      Get the name of the runnable.\n",
      " |\n",
      " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate output to the runnable.\n",
      " |\n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic output schema that depends on which\n",
      " |      configuration the runnable is invoked with.\n",
      " |\n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate output.\n",
      " |\n",
      " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'List[BasePromptTemplate]'\n",
      " |\n",
      " |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      " |      by calling invoke() with each input.\n",
      " |\n",
      " |  pick(self, keys: 'Union[str, List[str]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the dict output of this runnable.\n",
      " |      Returns a new runnable.\n",
      " |\n",
      " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |\n",
      " |  stream(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of stream, which calls invoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |\n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of transform, which buffers input and then calls stream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a runnable, returning a new Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base runnable\n",
      " |              and its fallbacks must accept a dictionary as input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |\n",
      " |  with_listeners(self, *, on_start: 'Optional[Listener]' = None, on_end: 'Optional[Listener]' = None, on_error: 'Optional[Listener]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      on_start: Called before the runnable starts running, with the Run object.\n",
      " |      on_end: Called after the runnable finishes running, with the Run object.\n",
      " |      on_error: Called if the runnable throws an error, with the Run object.\n",
      " |\n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |\n",
      " |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original runnable on exceptions.\n",
      " |\n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
      " |                                   between retries\n",
      " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original runnable on exceptions.\n",
      " |\n",
      " |  with_types(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  InputType\n",
      " |      The type of input this runnable accepts specified as a type annotation.\n",
      " |\n",
      " |  OutputType\n",
      " |      The type of output this runnable produces specified as a type annotation.\n",
      " |\n",
      " |  config_specs\n",
      " |      List configurable fields for this runnable.\n",
      " |\n",
      " |  input_schema\n",
      " |      The type of input this runnable accepts specified as a pydantic model.\n",
      " |\n",
      " |  output_schema\n",
      " |      The type of output this runnable produces specified as a pydantic model.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  name = None\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |\n",
      " |  __class_getitem__(...) from pydantic.v1.main.ModelMetaclass\n",
      " |      Parameterizes a generic class.\n",
      " |\n",
      " |      At least, parameterizing a generic class is the *main* thing this\n",
      " |      method does. For example, for some generic class `Foo`, this is called\n",
      " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |\n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo[T]: ...`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(out[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "397712fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseTool.invoke of Tool(name='google_search', description='Search Google for recent results.', func=<bound method GoogleSearchAPIWrapper.run of GoogleSearchAPIWrapper(search_engine=<googleapiclient.discovery.Resource object at 0x7f4c285341d0>, google_api_key='AIzaSyDPtBvSXiWGW5I6vPiaAL9LTip2AUX_MsY', google_cse_id='318aad6b62ac04395', k=10, siterestrict=False)>)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[3].invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadfae9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd152f43",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07836214",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"google_search\",\n",
    "    description=\"Search Google for recent results.\",\n",
    "    func=search.run,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0064bfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Tool in module langchain_core.tools object:\n",
      "\n",
      "class Tool(BaseTool)\n",
      " |  Tool(name: 'str', func: 'Optional[Callable]', description: 'str', *, args_schema: Optional[Type[pydantic.v1.main.BaseModel]] = None, return_direct: bool = False, verbose: bool = False, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, handle_tool_error: Union[bool, str, Callable[[langchain_core.tools.ToolException], str], NoneType] = False, handle_validation_error: Union[bool, str, Callable[[pydantic.v1.error_wrappers.ValidationError], str], NoneType] = False, coroutine: Optional[Callable[..., Awaitable[str]]] = None) -> None\n",
      " |\n",
      " |  Tool that takes in function or coroutine directly.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Tool\n",
      " |      BaseTool\n",
      " |      langchain_core.runnables.base.RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.v1.main.BaseModel\n",
      " |      pydantic.v1.utils.Representation\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, name: 'str', func: 'Optional[Callable]', description: 'str', **kwargs: 'Any') -> 'None'\n",
      " |      Initialize tool.\n",
      " |\n",
      " |  async ainvoke(self, input: 'Union[str, Dict]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Any'\n",
      " |      Default implementation of ainvoke, calls invoke from a thread.\n",
      " |\n",
      " |      The default implementation allows usage of async code even if\n",
      " |      the runnable did not implement a native async version of invoke.\n",
      " |\n",
      " |      Subclasses should override this method if they can run asynchronously.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  from_function(func: 'Optional[Callable]', name: 'str', description: 'str', return_direct: 'bool' = False, args_schema: 'Optional[Type[BaseModel]]' = None, coroutine: 'Optional[Callable[..., Awaitable[Any]]]' = None, **kwargs: 'Any') -> 'Tool' from pydantic.v1.main.ModelMetaclass\n",
      " |      Initialize tool from a function.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  args\n",
      " |      The tool's input arguments.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {'coroutine': 'Optional[Callable[..., Awaitable[str]...\n",
      " |\n",
      " |  __class_vars__ = set()\n",
      " |\n",
      " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
      " |\n",
      " |  __custom_root_type__ = False\n",
      " |\n",
      " |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True}\n",
      " |\n",
      " |  __fields__ = {'args_schema': ModelField(name='args_schema', type=Optio...\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __include_fields__ = None\n",
      " |\n",
      " |  __parameters__ = ()\n",
      " |\n",
      " |  __post_root_validators__ = [(False, <function BaseTool.raise_deprecati...\n",
      " |\n",
      " |  __pre_root_validators__ = []\n",
      " |\n",
      " |  __private_attributes__ = {'_lc_kwargs': ModelPrivateAttr(default=Pydan...\n",
      " |\n",
      " |  __schema_cache__ = {}\n",
      " |\n",
      " |  __signature__ = <Signature (name: 'str', func: 'Optional[Callabl...l[C...\n",
      " |\n",
      " |  __validators__ = {}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseTool:\n",
      " |\n",
      " |  __call__(self, tool_input: 'str', callbacks: 'Callbacks' = None) -> 'str'\n",
      " |      Make tool callable.\n",
      " |\n",
      " |  async arun(self, tool_input: 'Union[str, Dict]', verbose: 'Optional[bool]' = None, start_color: 'Optional[str]' = 'green', color: 'Optional[str]' = 'green', callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'Any'\n",
      " |      Run the tool asynchronously.\n",
      " |\n",
      " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      The tool's input schema.\n",
      " |\n",
      " |  invoke(self, input: 'Union[str, Dict]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Any'\n",
      " |      Transform a single input into an output. Override to implement.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: A config to use when invoking the runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details.\n",
      " |\n",
      " |      Returns:\n",
      " |          The output of the runnable.\n",
      " |\n",
      " |  run(self, tool_input: 'Union[str, Dict[str, Any]]', verbose: 'Optional[bool]' = None, start_color: 'Optional[str]' = 'green', color: 'Optional[str]' = 'green', callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, **kwargs: 'Any') -> 'Any'\n",
      " |      Run the tool.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from BaseTool:\n",
      " |\n",
      " |  __init_subclass__(**kwargs: 'Any') -> 'None' from pydantic.v1.main.ModelMetaclass\n",
      " |      Create the definition of the new tool class.\n",
      " |\n",
      " |  raise_deprecation(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseTool:\n",
      " |\n",
      " |  is_single_input\n",
      " |      Whether the tool only accepts a single input.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from BaseTool:\n",
      " |\n",
      " |  Config = <class 'langchain_core.tools.BaseTool.Config'>\n",
      " |      Configuration for this pydantic object.\n",
      " |\n",
      " |\n",
      " |  __orig_bases__ = (langchain_core.runnables.base.RunnableSerializable[t...\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |\n",
      " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
      " |\n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |\n",
      " |  to_json(self) -> 'Union[SerializedConstructor, SerializedNotImplemented]'\n",
      " |      Serialize the runnable to JSON.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  __repr_args__(self) -> Any\n",
      " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      " |\n",
      " |      Can either return:\n",
      " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      " |\n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  get_lc_namespace() -> List[str] from pydantic.v1.main.ModelMetaclass\n",
      " |      Get the namespace of the langchain object.\n",
      " |\n",
      " |      For example, if the class is `langchain.llms.openai.OpenAI`, then the\n",
      " |      namespace is [\"langchain\", \"llms\", \"openai\"]\n",
      " |\n",
      " |  is_lc_serializable() -> bool from pydantic.v1.main.ModelMetaclass\n",
      " |      Is this class serializable?\n",
      " |\n",
      " |  lc_id() -> List[str] from pydantic.v1.main.ModelMetaclass\n",
      " |      A unique identifier for this class for serialization purposes.\n",
      " |\n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.load.serializable.Serializable:\n",
      " |\n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |\n",
      " |      These attributes must be accepted by the constructor.\n",
      " |\n",
      " |  lc_secrets\n",
      " |      A map of constructor argument names to secret ids.\n",
      " |\n",
      " |      For example,\n",
      " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
      " |\n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |\n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |\n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |\n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |\n",
      " |  dict(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False) -> 'DictStrAny'\n",
      " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
      " |\n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> str\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |\n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
      " |\n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |\n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |\n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |\n",
      " |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __fields_set__\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.utils.Representation:\n",
      " |\n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |\n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __repr_name__(self) -> str\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |\n",
      " |  __repr_str__(self, join_str: str) -> str\n",
      " |\n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |\n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |\n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |\n",
      " |  async abatch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
      " |\n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |\n",
      " |  assign(self, **kwargs: 'Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the dict output of this runnable.\n",
      " |      Returns a new runnable.\n",
      " |\n",
      " |  async astream(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of astream, which calls ainvoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |\n",
      " |  astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1']\", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      [*Beta*] Generate a stream of events.\n",
      " |\n",
      " |      Use to create an iterator over StreamEvents that provide real-time information\n",
      " |      about the progress of the runnable, including StreamEvents from intermediate\n",
      " |      results.\n",
      " |\n",
      " |      A StreamEvent is a dictionary with the following schema:\n",
      " |\n",
      " |      - ``event``: **str** - Event names are of the\n",
      " |          format: on_[runnable_type]_(start|stream|end).\n",
      " |      - ``name``: **str** - The name of the runnable that generated the event.\n",
      " |      - ``run_id``: **str** - randomly generated ID associated with the given execution of\n",
      " |          the runnable that emitted the event.\n",
      " |          A child runnable that gets invoked as part of the execution of a\n",
      " |          parent runnable is assigned its own unique ID.\n",
      " |      - ``tags``: **Optional[List[str]]** - The tags of the runnable that generated\n",
      " |          the event.\n",
      " |      - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the runnable\n",
      " |          that generated the event.\n",
      " |      - ``data``: **Dict[str, Any]**\n",
      " |\n",
      " |\n",
      " |      Below is a table that illustrates some evens that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
      " |      +======================+==================+=================================+===============================================+=================================================+\n",
      " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | {\"generations\": [...], \"llm_output\": None, ...} |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_stream       | some_tool        | {\"x\": 1, \"y\": \"2\"}              |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_chunk   | [retriever name] | {documents: [...]}              |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | {documents: [...]}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |\n",
      " |      Here are declarations associated with the events shown above:\n",
      " |\n",
      " |      `format_docs`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          def format_docs(docs: List[Document]) -> str:\n",
      " |              '''Format the docs.'''\n",
      " |              return \", \".join([doc.page_content for doc in docs])\n",
      " |\n",
      " |          format_docs = RunnableLambda(format_docs)\n",
      " |\n",
      " |      `some_tool`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          @tool\n",
      " |          def some_tool(x: int, y: str) -> dict:\n",
      " |              '''Some_tool.'''\n",
      " |              return {\"x\": x, \"y\": y}\n",
      " |\n",
      " |      `prompt`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          template = ChatPromptTemplate.from_messages(\n",
      " |              [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |\n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |\n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v1\")\n",
      " |          ]\n",
      " |\n",
      " |          # will produce the following events (run_id has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          version: The version of the schema to use.\n",
      " |                   Currently only version 1 is available.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |          include_names: Only include events from runnables with matching names.\n",
      " |          include_types: Only include events from runnables with matching types.\n",
      " |          include_tags: Only include events from runnables with matching tags.\n",
      " |          exclude_names: Exclude events from runnables with matching names.\n",
      " |          exclude_types: Exclude events from runnables with matching types.\n",
      " |          exclude_tags: Exclude events from runnables with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the runnable.\n",
      " |              These will be passed to astream_log as this implementation\n",
      " |              of astream_events is built on top of astream_log.\n",
      " |\n",
      " |      Returns:\n",
      " |          An async stream of StreamEvents.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |\n",
      " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a runnable, as reported to the callback system.\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |\n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |\n",
      " |      The jsonpatch ops can be applied in order to construct state.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the runnable.\n",
      " |          config: The config to use for the runnable.\n",
      " |          diff: Whether to yield diffs between each step, or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |\n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of atransform, which buffers input and calls astream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |  batch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |\n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying runnable uses an API which supports a batch mode.\n",
      " |\n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'Type[BaseModel]'\n",
      " |      The type of config this runnable accepts specified as a pydantic model.\n",
      " |\n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |\n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |\n",
      " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
      " |      Return a graph representation of this runnable.\n",
      " |\n",
      " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
      " |      Get the name of the runnable.\n",
      " |\n",
      " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate output to the runnable.\n",
      " |\n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic output schema that depends on which\n",
      " |      configuration the runnable is invoked with.\n",
      " |\n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate output.\n",
      " |\n",
      " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'List[BasePromptTemplate]'\n",
      " |\n",
      " |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      " |      by calling invoke() with each input.\n",
      " |\n",
      " |  pick(self, keys: 'Union[str, List[str]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the dict output of this runnable.\n",
      " |      Returns a new runnable.\n",
      " |\n",
      " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this runnable with another object to create a RunnableSequence.\n",
      " |\n",
      " |  stream(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of stream, which calls invoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |\n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of transform, which buffers input and then calls stream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a runnable, returning a new Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base runnable\n",
      " |              and its fallbacks must accept a dictionary as input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |\n",
      " |  with_listeners(self, *, on_start: 'Optional[Listener]' = None, on_end: 'Optional[Listener]' = None, on_error: 'Optional[Listener]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      on_start: Called before the runnable starts running, with the Run object.\n",
      " |      on_end: Called after the runnable finishes running, with the Run object.\n",
      " |      on_error: Called if the runnable throws an error, with the Run object.\n",
      " |\n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |\n",
      " |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original runnable on exceptions.\n",
      " |\n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
      " |                                   between retries\n",
      " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original runnable on exceptions.\n",
      " |\n",
      " |  with_types(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  InputType\n",
      " |      The type of input this runnable accepts specified as a type annotation.\n",
      " |\n",
      " |  OutputType\n",
      " |      The type of output this runnable produces specified as a type annotation.\n",
      " |\n",
      " |  config_specs\n",
      " |      List configurable fields for this runnable.\n",
      " |\n",
      " |  input_schema\n",
      " |      The type of input this runnable accepts specified as a pydantic model.\n",
      " |\n",
      " |  output_schema\n",
      " |      The type of output this runnable produces specified as a pydantic model.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  name = None\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |\n",
      " |  __class_getitem__(...) from pydantic.v1.main.ModelMetaclass\n",
      " |      Parameterizes a generic class.\n",
      " |\n",
      " |      At least, parameterizing a generic class is the *main* thing this\n",
      " |      method does. For example, for some generic class `Foo`, this is called\n",
      " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |\n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo[T]: ...`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "305b73c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tool.run(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "999e462e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1681"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
